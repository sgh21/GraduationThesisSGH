\documentclass{Diploma}
%引入需要的宏包
\usepackage{amsmath}
%设置封面信息
\SetDepartment{机械工程系}
\SetMajor{机械工程}
\SetTitle{手机BTB插接件高精度高鲁棒位姿检测技术研究}
\SetAuthor{申广辉}
\SetInstructor{吴丹}{教授} 
% \SetJointInstructor{副指导}{副教授}
\begin{document}
% 摘要
\ChsAbstract 零件在手位姿检测是手机BTB连接器自动插接过程中弥补抓取误差、实现精确插入控制的关键技术环节。本文面向手机装配生产线实际应用需求，提出了一种高精度、高鲁棒性的相对位姿检测算法，算法通过视触觉协同训练，推理阶段仅依赖视觉输入即可实现精准鲁棒的位姿估计。在此基础上，设计并开发了一套手机BTB连接器全流程批量自动装配系统，实现了识别抓取、检测对准和插入控制的核心步骤，并通过算法对比实验和综合插接实验系统性地验证了所提方法的有效性和优越性。本文完成主要工作及成果如下：

对手机BTB连接器插接任务进行了数学定义和分析，论证了在手位姿检测可转化为相对位姿检测问题的理论依据，并基于数据驱动思想，分析了从视觉输入到相对位姿输出的核心步骤，并以此为指导和设计了基于掩码自编码特征提取、交叉注意力特征对比的相对位姿检测网络框架。通过初步训练和验证，证明了网络框架的可行性。

为应对实际应用中面临的光照波动、背景环境不一致、零件批次间的表面质量变化和新型号泛化的挑战，进行了针对性的策略设计：设计了光照对齐模块对光照波动进行标准化处理予以补偿；使用掩码生成模块，预先分割出零件区域，并提高在编码和推理过程中零件本体区域的特征权重；创新性引入触觉传感器，通过触觉采样点云监督网络训练，提升了网络在视觉特征退化时捕捉鲁棒特征的能力。通过纵向消融实验验证了各模块的有效性和必要性，通过横向对比实验验证了所提出方法相比于已有方法在精度和鲁棒性上的综合优势。

搭建并实现了手机BTB连接器全流程批量自动装配系统，实现了识别定位和抓取、检测对准与对准和柔顺插入控制的核心步骤。通过ROS系统对算法模块进行集成，开展了综合插接实验，在对9种型号手机BTB连接器的50轮插接实验中，插接成功率高达$100\%$，并通过监测插入力和力矩，验证了插接过程的安全性和可靠性，满足实际工业应用的批量自动装配需求。

\ChsKeywords{手机BTB连接器；深度学习；视触觉协同；相对位姿检测；自动装配}

\EngAbstract
In-hand pose detection is a critical technology for compensating grasping errors and achieving precise insertion control in the automated assembly of mobile phone Board-to-Board (BTB) connectors. Aiming at practical demands of mobile phone assembly lines, this paper proposes a highly accurate and robust relative pose detection algorithm. The proposed method utilizes visual-tactile coordinated training, but relies solely on visual inputs during inference, thus enabling accurate and robust pose estimation. Furthermore, a complete automatic batch assembly system for mobile phone BTB connectors was designed and developed, covering core processes including recognition and grasping, detection and alignment, and compliant insertion control. Extensive comparative experiments and integrated insertion tests systematically verified the effectiveness and superiority of the proposed approach.

The main contributions and achievements of this paper are as follows:

Firstly, a mathematical definition and analysis of the mobile phone BTB connector insertion task was presented, demonstrating the theoretical feasibility of transforming the in-hand pose detection problem into a relative pose detection task. Driven by data-centric methods, the core process from visual inputs to relative pose outputs was analyzed, and guided by this, a relative pose detection network framework based on mask autoencoder feature extraction and cross-attention feature comparison was designed. Preliminary training and validation verified the feasibility and effectiveness of the proposed network framework.

Secondly, targeted strategies were developed to address practical challenges such as illumination fluctuations, inconsistent background environments, variations in surface quality between batches, and generalization to new connector models. An illumination alignment module was designed to standardize input illumination conditions. A mask generation module was introduced to pre-segment connector regions and enhance feature weighting during encoding and inference. Additionally, tactile sensors were innovatively employed to generate point clouds for supervised training, significantly improving the robustness of visual feature extraction when visual input quality deteriorates. Ablation studies demonstrated the necessity and effectiveness of each module, while comparative experiments validated the comprehensive advantages of the proposed method in accuracy and robustness over existing approaches.

Finally, a full-process automatic batch assembly system for mobile phone BTB connectors was established, implementing critical steps including recognition, grasping, detection alignment, and compliant insertion control. The system integration was performed through the Robot Operating System (ROS). Comprehensive insertion tests conducted across 50 trials with 9 connector models achieved a $100\%$ insertion success rate. By monitoring insertion forces and torques, the safety and reliability of the insertion process were verified, fully satisfying the practical industrial requirements for automated batch assembly.

\EngKeywords{Mobile phone BTB connector; Deep learning; Visual-tactile collaboration; Relative pose detection; Automated assembly}

% 目录及图表清单
\ListOfContents

\StartMainText% 正文开始

\chapter[preface]{引言}
\section{研究背景与意义}
随着智能科技和信息化产业的飞速发展，智能手机已成为人们生活中不可或缺的重要设备。据2024年Omdia发布的《智能手机机型市场跟踪报告》显示，在2024年上半年，仅iPhone 15ProMax全球出货量就高达2180万部。所有型号手机每季度全球出货量更是高达2.88亿部。而在手机设计和制造过程中，其硬件部件需要经过精密装配以确保性能和质量。其中，作为连接手机主板与屏幕、电池等功能模块的核心零件，BTB（Board-to-Board）连接器的插接质量对整个装配效果至关重要。

当前，BTB连接器的装配过程主要依赖人工操作。然而，随着手机部件微型化趋势的加剧，传统人工操作逐渐暴露出产品一致性差和生产效率低的问题。因此，迫切需要使用自动化技术实现BTB连接自动插接，提高生产效率和产品质量\cite{sahoo2022smart}。
\InsertFigure[ManualConnection]{}{人工插接示意图}{preface/ManualConnection.png}%

本课题旨在面向手机装配生产流水线，应用自动化技术，开发手机BTB连接器全流程批量自动装配系统，实现识别抓取、检测对准和插入控制核心步骤，解决实际生产需求，提高装配效率。

手机BTB连接器批量自动插接可细分为识别抓取、检测对准和插入控制三个步骤\cite{jiang2022state}。其中识别抓取受到机械臂绝对定位误差和视觉标定误差的制约，存在不可避免的毫米级误差，因此对于更精密的装配任务，必须进行补偿和修正。在工业流水线中，一般会选择进行在手位姿检测确定零件的对手位姿，随后在完成对准之后进行插入。基于位置控制的插入控制策略机械臂不会主动对外感知，如果检测对准超出容差，会导致零件损坏。为了保证插入过程安全可靠，通常采用阻抗/导纳控制等力控策略，进行柔顺插入。在以上三个步骤中，识别抓取相对简单，并且由于毫米级误差不可避免，因此无需过度苛求检测的精度，现有的基于Yolo分割的算法足以满足精度需求，高精度高鲁棒的位姿检测算法既可以减轻柔顺插入控制策略的开发难度，也是插接可靠性的重要保障。因此，开发具有高精度和高鲁棒的位姿检测算法具有重要意义。
\section{国内外研究现状}
本论文主要聚焦于手机BTB连接器的位姿检测算法。位姿检测环节的目的是消除由于抓取产生的误差，检测零件在图像中的相对位姿，再通过标定转换获得零件对机器人末端工具坐标系的相对位姿或者对于机器人基坐标系的绝对位姿。

现有的位姿检测算法主要有基于特征工程的传统视觉算法、结合深度学习的传统视觉算法、多模态深度学习算法和视觉伺服控制算法四类。下面将对这几类算法按照技术发展脉络进行介绍与分析。
\subsection{基于特征工程的传统视觉}
此类方法的核心思想是基于先验知识，归纳设计零件用于定位的特征，如颜色、边缘等信息，随后借助边缘检测、特征拟合等手段确定零件在图像中的局部位姿，再通过标定转换，获得其全局位姿。因此定位精度取决于零件特征拟合质量和标定转换精度。

Wang等针对3C装配领域的薄片零件对象，从2D图像中提取薄片零件的边缘轮廓，再根据零件的CAD数模和当前估计位姿，实现3D到2D的重投影，将重投影边缘和提取边缘进行误差优化，通过优化过程从而获得平移误差仅$0.03\,\mathrm{mm}$，旋转误差仅$0.1^{\circ}$的高精度六维位姿。此方法通过改善特征拟合的质量构建了优化过程，但其优化过程建立依赖于CAD数模和零件边缘轮廓的解析表达式，具有较强的先验依赖，因此对于不同的零件较难拓展\cite{wang2025high}。
\InsertFigure[Wang]{}{基于特征工程的传统视觉算法示意图\cite{wang2025high}}{preface/Wang.png}%

Jiang等为了减小机械臂的绝对定位误差导致的标定转换误差，在执行器末端增设标志点，使用固定的全局相机和在机械臂末端的局部相机构成两级视觉系统，进行动态坐标标定和全局校准，完成了$0.1\,\mathrm{mm}$的轴孔装配任务，提高了标定转换环节的精度\cite{jiang2020measurement}。

此类方法的优势在于其精度高，方法简单并且计算开销小，因此在工业界应用最为广泛。但基于传统视觉的方法很容易受到光照变化、油渍和视角遮挡等问题的影响，不够鲁棒，并且算法的精度依赖于特征提取的质量，特征难提取或特征点少时精度明显下降，甚至存在检测失败的情况。另外，对于不同零件的特征需要进行精巧设计，难以泛化。
\subsection{传统视觉结合深度学习}
为了提高特征提取的鲁棒性，有学者提出传统视觉和深度学习结合的方法。此类方法的核心思想是借助深度学习的特征提取能力，完成主要特征提取或实例分割，给出感兴趣区域，再利用传统算法对特征区域进行进一步的特征分析，完成由粗到精的定位过程。

Mou等使用Yolo结合传统算法的方法，其引入了两个Yolo模型分别完成连接器的识别，给出连接器的感兴趣区域和连接器针脚分割，给出连接器的针脚位置。得益于Yolo成熟的框架，在Yolo预训练模型基础上实现上述功能仅需要几千组数据，这对于神经网络训练来说是属于很小的量级。在给出针脚位置后，其根据先验对针脚位置流行进行校正，使用主成分分析，获取流形主方向，完成了平移误差$0.36\,\mathrm{mm}$，旋转误差$0.44^{\circ}$的位姿检测\cite{fangli2022pose}。
\InsertFigure[Mou]{}{传统视觉结合深度学习算法示意图\cite{fangli2022pose}}{preface/Mou.png}%

此类方法能一定程度上提高位姿检测的鲁棒性，问题在于后续传统视觉的处理使其仍然存在依赖先验和泛化难的问题，并且精度仍然较大程度上受最终特征提取的制约。
\subsection{多模态深度学习}
深度学习的快速发展使得通过数据驱动的方法在鲁棒性上有了飞速的提升，并且深度学习的特征提取能力使得其不再依赖于特征设计，能够自动从数据中学习到特征，因此在泛化性上也表现出了巨大的潜力。此类方法的核心思想在于，使用神经网络对输入进行特征编码，完成特征提取，随后对特征进行相应任务回归，获得回归输出，并和标签进行比对，通过比对的损失回传，更新网络参数，优化特征提取和回归过程\cite{yu2019siamese}。为了克服插接任务的视角遮挡，并充分训练网络的特征提取能力，有学者提出了多模态深度学习的方法，通常分为多视角输入和多类型传感输入两类。

Yang等引入数字孪生技术，通过识别调姿平台的角点，获取单应性矩阵创建孪生环境，并在孪生环境中采集无遮挡视角视图，从而构成多视角输入。多视角视图输入给卷积神经网络进行特征层拼接，完成数据融合，输出估计位姿。此方法解决了插入过程对准视角遮挡的问题，完成了BTB连接器零件插接任务\cite{yang2023digital}。
\InsertFigure[Yang]{}{多模态深度学习算法示意图\cite{yang2023digital}}{preface/Yang.png}%

Li等则引入触觉传感器，搭建了稳健视触觉网络，通过补偿模块应对单一模态缺失的问题，因此可以保证当视觉受到干扰或质量变差时依然能够凭借触觉信息完成对准，提高了位姿检测的鲁棒性\cite{li2024v}。

此类方法对光照、遮挡的鲁棒性好，精度相对较高。但是其训练时间和数据获取成本相对更高，并且其研究任务仍然停留在数据集测试层面，没有考虑到实际部署时存在的问题，比如其在数据采集时一般采用机械臂与零件协同运动的策略，这导致数据中零件和背景同步运动，但实际部署时机械臂末端在固定拍照，因此背景始终保持不动，这种差异会使得基于学习的视觉位姿检测在实际部署时精度显著下降。
\subsection{视觉伺服控制}
除上述三种通过分析单帧图像获得位姿外，还有一类另辟蹊径的方法，既然单次检测不够精准，其不直接测量轴孔的绝对位姿，而是将传感器作为伺服系统，在插入过程中进行实时测量，采取定位误差补偿的策略，完成对准，此类方法的核心在于伺服控制和搜索策略。

伺服传感器一般选择视觉传感器或力觉传感器，但视觉传感器在插入过程往往存在视角遮挡，图像质量较差，而力传感器数据则不直观，难以获得从力到位移的直接映射关系。因此，在伺服控制策略的最新研究中，往往引入强化学习智能体，采用感知、调整、评价的迭代步骤进行策略学习，通过数据驱动的方式来进行实时的反馈和调整，最终获得能自主调整和搜索的智能体。

Zhao等使用先粗后精的视觉伺服引导，首先使用视觉传感器完成粗略位姿检测完成粗定位，缩小搜索空间，随后在精确对准环节，将视觉作为伺服系统，训练强化学习智能体进行轴孔对准，完成了USB连接器的插接任务，给出了连接器装配新思路\cite{zhao2023learning}。
\InsertFigure[Zhao]{}{视觉伺服控制算法示意图\cite{zhao2023learning}}{preface/Zhao.png}%

此类方法针对不同尺寸的轴孔迁移快，泛化强，但基于强化学习的方法训练往往依赖于仿真器。一方面，对于低刚度并且在插接过程存在弹性变形的对象，难仿真建模，因此训练难以开展。另一方面，在仿真中开展模型训练存在仿真到现实迁移的困难，这依然是制约强化学习在实际工程应用最大的困难。
\subsection{位姿检测技术路线梳理}
针对以上方法进行总结，位姿技术发展路线如\ref{fig:DetectionTech}所示，基于特征工程的传统视觉精度高，方法简单，方便部署和应用，但其对环境鲁棒性和对不同零件泛化性差。引入深度学习加强的传统视觉其泛化性和鲁棒性得到提升，但带来了和精度下降问题。而使用多模态深度学习在精度和鲁棒性上都表现出了巨大潜力，但目前研究没有考虑实际部署和训练的差异。而使用伺服系统的策略，需要引入强化学习智能体，但其针对难以仿真建模的对象的无法训练和从训练到现实的迁移的问题还没有成熟的解决方案。
\InsertFigure[DetectionTech]{1.0\textwidth}{检测对准技术发展路线}{preface/DetectionTech.png}

针对本课题研究的手机BTB插接对象的特，现有方案在应对手机BTB插接对象时仍存在不足。因此，本课题的首要目标是开发一种兼具高精度、强泛化性、低成本，且无需依赖复杂模型的检测对准算法。
\section{研究目标与内容}
\subsection{研究目标}
本课题面向手机BTB连接器插接的应用需求，搭建全流程自动插接系统，实现识别抓取、位姿检测和柔顺插入控制。并且解决多种类型BTB连接器的批量自动化插接中的位姿检测问题，重点提升高精度、高鲁棒性与强泛化性。具体目标包括：
\begin{itemize}
  \item 高精度：实现位姿检测的平移精度达到$0.1\,\mathrm{mm}$，旋转精度达到$1.5^{\circ}$； 
  \item 高鲁棒：能够适应光照波动、背景不一致、批次间零件表面质量变化等复杂工况；
  \item 强泛化性：适配针脚数量范围为24P至60P，针脚间距为$0.35\,\mathrm{mm}$至$0.4\,\mathrm{mm}$的多种BTB连接器类型； 
\end{itemize}
\subsection{研究内容}
本文的主要研究内容可分为以下几个方面：
\begin{enumerate} 
  \item 在手位姿检测任务定义：明确手机BTB连接器在手位姿检测任务的数学定义，论证在手位姿检测可转化为相对位姿检测问题的理论依据，分析从视觉输入到相对位姿输出的核心步骤。
  \item 基于数据驱动的位姿检测网络框架设计：根据任务阶段设计数据驱动的视觉检测网络框架，给出网络的整体结构和主要模块设计，给出数据集构建与训练方法。
  \item 针对实际应用挑战的详细模块设计：针对标准网络框架，分析其实际应用存在的挑战和问题，并给出问题解决方案，给出视触觉协同训练的位姿检测网络的结构，详细模块设计，实验对比与结果分析。
  \item 综合实验与分析：搭建实机硬件系统，给出系统数学模型与坐标系标定方法，给出软件系统框架设计，实现识别抓取算法模块、检测对准算法模块和柔顺插入控制算法模块的设计与实现，给出综合实验效果与分析。
\end{enumerate}
\section{社会责任分析}
本课题面向手机BTB连接器的自动化插接需求，所研究的高精度位姿检测和柔顺装配技术，在具体工业场景中具有显著的实用价值和社会效益：

\begin{itemize}
\item \textbf{提升产品一致性与减少报废率：} 在当前人工插接中，由于定位误差和插接不当导致的针脚弯折或连接器损坏问题较为常见，影响产品一致性。本课题通过引入高精度视觉检测和柔顺插入控制技术，有望显著降低装配过程中的不良率和返工次数，减少资源浪费。

\item \textbf{降低重复性劳动带来的健康风险：} 长时间进行细微装配操作容易导致工人眼疲劳、手腕损伤等职业病。实现插接过程自动化可减轻人工负担，改善生产一线工人的工作环境和身体健康风险。

\item \textbf{增强中小企业的技术可及性：} 本课题强调“高精度、强泛化、低成本”的设计原则，目标是实现不依赖昂贵设备和复杂建模的解决方案。这对于资金与技术储备有限的中小型制造企业来说更具可推广性，具有现实落地的可能。
\end{itemize}
\section{本章小结}
本章围绕手机BTB连接器插接自动化背景，阐述了本文的动因与必要性，并系统综述了国内外在位姿检测领域的主流方法，从技术脉络上涵盖基于特征工程的传统视觉、融合深度学习的混合方法、多模态深度学习方法与视觉伺服控制策略，剖析其优劣与适用边界，指出现有技术在泛化性、鲁棒性及实际部署方面的不足。

在此基础上，明确了本文的技术目标，包括实现高精度、高鲁棒性与强泛化能力的相对位姿检测网络框架设计，针对实际应用挑战的详细模块设计和支持多种类型BTB连接器的批量自动插接系统构建。

\chapter[framework]{数据驱动的位姿检测网络框架设计}
在手位姿检测是实现手机BTB连接器插接的核心环节，本章将对在手位姿检测问题进行数学定义和分析，给出将在手位姿检测转化为相对位姿检测的理论依据。最后，给出针对该任务的深度学习网络设计方法和基础框架，以及针对网络框架的数据集构建和训练方法。

\section{相对位姿检测问题描述与难点}

\subsection{问题描述}
视觉位姿检测是实现零件插接的关键环节，此环节的目的是消除由于抓取产生的误差，获得零件对工具坐标系末端的相对准确的位姿，从而引导引导机械臂在插入过程中对抓取误差进行补偿。

\InsertFigure[CoordinateSystem]{0.8\textwidth}{位姿检测坐标系统示意图}{framework/CoordinateSystem.png}%

实际的坐标系定义如\ref{fig:CoordinateSystem}所示，世界坐标系为$T_{world}$，机械臂末端工具坐标系为$T_{tool}$，相机坐标系为$T_{camera}$，零件坐标系为$T_{part}$。位姿检测的任务是获得零件坐标系$T_{part}$相对于机械臂末端工具坐标系$T_{tool}$的位姿$T_{part}^{tool}$，即\eqref{eq:pose1}所示。
\begin{equation}
  T_{part}^{tool} =T_{world}^{tool} \cdot T_{camera}^{world} \cdot T_{part}^{camera} \label{eq:pose1}
\end{equation}

其中$T_{camera}^{world}$是相机坐标系在世界坐标系下的位姿，通过手眼标定获取，在多次抓取时保持不变，$T_{world}^{tool}$ 是机械臂末端工具坐标系相对基座标系的位姿，可以通过机械臂的编码器读数获取。因此，$T_{part}^{tool}$的计算可以转化为$T_{part}^{camera}$的计算，即从像素坐标系中提取出零件的位姿信息，再通过内参矩阵转换到相机坐标系下。

\InsertFigure[PictureCoordinateSystem]{0.4\textwidth}{像素坐标系和相机坐标系}{framework/PictureCoordinateSystem.png}%

为了使用深度学习进行相对位姿检测，需要获取每次抓取时零件在相机坐标系下的位姿真值，使用已有的算法获取零件位姿真值会引入预测误差，因此为了获得更为准确的真值，需要通过机械臂单次抓取，并进行相对移动，根据机械臂末端的相对移动，给出零件相对于相机的相对位移真值，其数学关系如\eqref{eq:pose2}所示。
\begin{equation} \label{eq:pose2}
\begin{aligned} 
  T_{part}^{tool}(t) &= T_{world}^{tool}(t) \cdot T_{camera}^{world} \cdot T_{part}^{camera}(t)  \\
  T_{part}^{tool}(t+1) &= T_{world}^{tool}(t+1) \cdot T_{camera}^{world} \cdot T_{part}^{camera}(t+1) \\
  T_{part}^{tool}(t) &= T_{part}^{tool}(t+1) 
\end{aligned}
\end{equation}

\eqref{eq:pose2}整理，可得：
\begin{equation} \label{eq:pose3}
    (T_{world}^{tool}(t+1))^{-1} \cdot T_{world}^{tool}(t) \cdot T_{camera}^{world} = T_{camera}^{world} \cdot T_{part}^{camera}(t+1) \cdot (T_{part}^{camera}(t))^{-1}
\end{equation}

\eqref{eq:pose3}左侧为机械臂末端工具坐标系在世界坐标系下的相对位姿，右侧为零件在相机坐标系下的相对位姿。由于机械臂末端工具坐标系在世界坐标系下的位姿是通过机械臂编码器读数获得的，精度较高。因此，可以使用机械臂末端工具坐标系在世界坐标系下的相对位姿，作为零件在相机坐标系下的相对位姿的真值，并将其作为网络的标签进行训练。

于是，使用深度学习进行相对位姿检测的任务可以转化为，输入机械臂相对移动的两张图片，输出两张图片相对相机坐标系的相对位姿，其真值为机械臂末端工具坐标系在世界坐标系下的相对位姿。
\subsection{任务难点}
手机BTB连接器的位姿检测任务面临多重技术挑战，主要体现在以下几个方面：

\begin{itemize}
\item \textbf{连接器类型多样，适应性要求高：} 即使在同一型号的手机中，也可能包含多种不同形态与结构的BTB连接器。这种多样性要求检测算法具备较强的泛化能力与鲁棒性，能够在不同种类的连接器之间实现稳定识别与位姿估计。此外，在实际生产环境中，手机型号更新换代频繁，检测系统必须具备良好的迁移能力，能够在无需大量重训练的情况下快速适应新型号的连接器结构与外观特征。
\item \textbf{结构形貌复杂，视觉特征不显著：} 相较于传统的规则形插头，BTB连接器表面形貌复杂、外轮廓不规则，且缺乏统一的颜色特征。这种结构复杂性导致其难以通过几何建模或传统基于色彩的图像分割方法进行有效识别。此外，连接器表面通常具有细微的纹理与微结构，这些微结构在图像中难以显著呈现，对图像分辨率和网络提取细节能力提出了更高要求。同时，这些结构也对后续基于力控的柔顺插入控制策略构成了挑战。
\item \textbf{器件微型化，检测精度要求极高：} 手机BTB连接器属于高精密微型器件，其针脚间距通常仅为$0.35--0.4\,\mathrm{mm}$，单个针脚宽度甚至只有$0.2\,\mathrm{mm}$。在视觉图像中，这类特征往往只占据极小区域，导致其目标区域在分割与检测是容易漏检。此外，其装配过程对位姿精度的要求高，通常需控制在±$0.1\,\mathrm{mm}$的位置误差和$1.5^{\circ}$以内的角度误差范围内。这对图像采集系统的分辨率、图像处理算法的检测精度及整个检测系统的稳定性提出了极高要求。
\end{itemize}
\section{端到端的网络总体框架设计}
在对框架进行介绍之前，需要再次明确基于深度学习的位姿检测的核心任务。网络的输入为有相对位移的图片，输出为估计的相对位姿，使用机器人末端工具坐标系的相对位姿作为训练的标签。为了完成该任务，网络需要完成以下步骤：

\begin{enumerate}
  \item \textbf{特征提取和编码：}输入的图片需要经过特征提取和编码，需要网络能够对信息进行抽象，提取图片中能表示零件位置的信息进行编码和数据降维，将数据从像素空间转化到特征空间。
  \item \textbf{特征对比：}网络的期望输出是两张图片的相对位移，因此需要进行“求差”的操作，对网络来说则是对特征向量进行对比。
  \item \textbf{任务回归：}根据输出任务，将对比的特征向量按照输出要求进行回归和维度转换，输出相对位姿。
  \item \textbf{损失计算和梯度回传：}根据输出的相对位姿和标签进行损失计算，回传梯度，更新网络参数。
\end{enumerate}

根据上述步骤和思想，设计了如\ref{fig:Network}所示的网络结构。该网络结构分为四个模块，分别是特征编码模块，特征对比模块，任务回归模块和损失计算模块。网络的工作流程为，输入两张通过单目相机采集的具有相对位姿的待检测图片，使用自掩码编码器对输入进行特征提取和编码，再使用交叉注意力对特征向量进行对比计算，然后根据预测任务，将对比向量进行降维，输出为三维预测位姿，最后使用平滑$L_{1}$范数计算预测输出和标签真值的差异，将梯度损失回传，完成一次参数更新。每个模块的设计思路和实现方法将在后续章节中详细介绍。
\InsertFigure[Network]{1.0\textwidth}{位姿检测框架示意图}{framework/Network.png}%

\section{端到端的网络主要模块结构设计}
\subsection{特征编码模块}
有效的特征编码是实现高精度位姿检测的关键，该模块要求能够对输入具有较强的抽象能力，对各种复杂背景和光照条件下的零件进行有效编码。除了强大的编码能力，相对位姿检测任务相较于常见的图像分类和实例分割任务还存在本质的不同。图像分类和实例分割任务要求编码器具有不变行性，即将输入进行旋转平移变换，输出保持不变，而相对位姿检测任务则要求编码器具有等变性，即输入进行旋转平移变换，输出也进行相应的旋转平移变换，其数学关系如\eqref{eq:transkeep}所以。因此，特征编码模块设计时需着重考虑以上两点，分别是强大的编码能力和等变性。
  \begin{equation} \label{eq:transkeep}
    \begin{aligned}
    f(T(x)) &= f(x) \\
    f(T(x)) &= T(f(x)) 
    \end{aligned}
  \end{equation}

  在图像处理领域，常见的编码器有两种，一种是以卷积神经网络（CNN）为基础的编码器，另一类是以视觉变换器（ViT）为基础的编码器。CNN在图像处理领域经过多年的发展，已经形成了成熟的网络结构和训练方法，具有较强的特征提取能力。其核心计算操作是使用卷积核对全图进行扫描计算，获得特征图，再进行池化操作增大感受野，并对输入进行压缩和降维，因此其具有较好的平移不变性，在图像分类领域有广泛的应用\cite{he2016deep}。但其全局感受能力较弱，等变性差。ViT则是近年来新兴的网络结构，它是变换器（Transformer）结构在视觉领域的推广\cite{dosovitskiy2020image}，Transformer编码器则是自注意力层、归一化层、前向层和残差连接为基础模块的编码结构，其自注意力机制在于捕获不同向量之间的潜在关联，归一化层是为了保证训练时的数值稳定，前向层通过全连接层对特征进行转译，残差连接保证了梯度回传、减少信息损失，使得网络可以有更大的深度，更强的编码能力\cite{vaswani2017attention}。但Transformer编码器针对词向量进行操作，因此ViT在使用Transformer编码器前，先将图片进行分块编码，将其转化为多个词向量，然后添加位置编码来保证全局位置关系，最后将其输入给多层Transformer编码器进行编码。一方面，Transformer编码器保持了全局位置关系，拥有全国感受野，能够更好捕捉全局信息，获得位置关系的同时，保证了其等变形的性质，另一方面，ViT的每一层都采用残差连接，使得其网络结构可以更深，具有更强的特征提取能力。

\begin{subfigures}[submodules]{主要模块设计}
  \SubFigure[FutureEncoder]{0.8\textwidth}{0.25\textwidth}{ViT编码器\cite{dosovitskiy2020image}}{framework/ViTa.png}%
  \SubFigure[ScaledMultiHeadAttention]{0.8\textwidth}{0.35\textwidth}{交叉注意力结构\cite{vaswani2017attention}}{framework/ViTb.png}%
  \SubFigure[MultiHeadAttention]{0.8\textwidth}{0.35\textwidth}{交叉注意力模块\cite{vaswani2017attention}}{framework/ViTc.png}%
\end{subfigures}

ViT编码器在特征提取能力和等变性上都具有较强的优势，因此本文选择ViT作为特征编码模块的基础结构。ViT编码器的结构如\ref{fig:FutureEncoder}所示，分块编码将输入图片进行分块处理，将其转化为多个词向量，位置编码则是为了保证全局位置关系，最后将其输入给多层Transformer编码器进行编码。使用的Transformer的主要结构参数如下：输入图片分辨率为224*224，分块尺寸为16*16，模块层数为12层，隐藏层的维度为768，多头注意力头数为12个，全连接层随机丢弃率为0.1，归一化方式采用层归一化。
\subsection{特征对比模块}
特征对比模块的主要任务是对两张图片的编码向量进行对比，从而形成对比向量，将特征向量的绝对位姿信息转化为相对位姿信息。因此，该模块设计时应该蕴涵差异计算的思想。

在深度学习领域，直接将特征向量做差会导致信息损失和数值不稳定，为了计算向量差异通常考虑两个向量的内积。在网络训练过程中，为了保证数值稳定，向量都是分布在0到1之间，因此，两个特征向量的模长是一致的，向量的内积大小只和向量的取向有关，向量的取向越相近，内积越大，向量相似度越高，因此可以用向量的内积来衡量向量的相似度，反过来其也可以反应两向量的差异。交叉注意力就是利用了这个思想，交叉注意力的计算过程如\ref{fig:ScaledMultiHeadAttention}所示。交叉注意力的计算过程分为三个步骤，首先将输入的两个特征向量进行线性变换，得到查询向量$Q$、键向量$K$和数值向量$V$，然后计算查询向量和键向量的内积，得到注意力权重矩阵$A$，最后将注意力权重矩阵进行SoftMax归一化和数值向量进行加权求和，得到输出特征向量，输出的特征向量即为所需对比向量。在实际使用时，在输入输出部分添加线性层用于转译，将两个对比向量进行拼接，得到最终的对比向量，减少信息损失。其结构如\ref{fig:MultiHeadAttention}所示。交叉注意力模块的主要参数设置如下：输入特征向量维度为768，输出特征向量维度为1536，交叉注意力头数为12，随机丢弃率为0.1，归一化方式采用层归一化。

\subsection{任务回归模块和损失计算模块}
任务回归模块的主要任务是将对比向量进行降维，输出相对位姿。由于相对位姿是一个三维向量，因此该模块的设计需要考虑到输出的维度和数值范围。

\InsertFigure[FC]{1.0\textwidth}{任务回归模块}{framework/FC.png}%

一般任务回归模块设计相对简单，采用全连接层进行线性转译即可，但由于ViT将每个分块编码成一个特征向量，因此输出的向量维度为$[B,N,L]$，其中$N$为分块数，$L$为特征向量维度，因此需要对各个通道先进行均值池化。另外，预测输出的范围需要和相对位姿的数值范围对齐，需要对输出进行归一化处理。为了保证输出的数值范围，使用了Tanh函数进行归一化处理，Tanh函数的输出范围为-1到1，将输出乘以一个常数$C$，使得输出的数值范围在$[-C,C]$之间。任务回归模块的主要参数设置如下：输入特征向量维度为1536，输出特征向量维度为3，随机丢弃率为0.1，归一化方式采用批归一化。其中，各层参数如\ref{fig:FC}所示。

在训练时，使用平滑$L_{1}$范数作为损失函数，计算标签真值和预测输出的差异，平滑$L_{1}$范数的计算方式如\eqref{eq:loss}所示。当输入较大时，平滑$L_{1}$范数等价于$L_2$范数，当输入较小时，平滑$L_{1}$范数等价于$L_{1}$范数，其优点在于相较于$L_1$范数其对异常值不敏感，在零点可导，能够有效避免异常值对训练的影响，同时，相较于$L_2$范数，其梯度性质更好，优化效果更好。
\begin{equation} \label{eq:loss}
  L_{smoothL1}(x) = \begin{cases}
    0.5x^2 & \text{if } |x| < 1 \\
    |x| - 0.5 & \text{otherwise}
  \end{cases}
\end{equation}

\section{数据集构建与训练}
\subsection{数据集构建}
为了训练网络，需要构建一个包含多种类型BTB连接器的位姿数据集。数据集的构建需要考虑到以下几个方面：
\begin{itemize}
  \item \textbf{数据集的多样性：} 一方面，数据集需要包含多种类型BTB连接器的位姿数据，另一方面，需要位姿多样，使得其能覆盖抓取误差范围保证数据集的多样性和代表性。
  \item \textbf{数据集的大小：} 数据集需要包含足够多的数据，保证网络的训练效果和泛化能力，但又要尽量减小数据采集的负担。
\end{itemize}

在数据采集时，使用机械臂进行数据采集，机械臂的末端安装了Epick吸盘用于抓取连接器零件，采集时，机械臂一次抓取，移动到位姿检测工位，并在一个平面内进行随机移动，平移移动范围为$[-4.5\,\mathrm{mm},4.\,\mathrm{mm}]$，旋转方向移动范围为$[-7.5^{\circ},7.5^{\circ}]$。采集多种类型BTB连接器的位姿数据，其中连接其的针脚数有24P，30P，34P，40P,60P五种，针脚间距有$0.35\,\mathrm{mm}$和$0.4\,\mathrm{mm}$两种，共十种连接器类型，每种类型采集图片共150张，共1500张图片。为了使得网络能够对光照有一定的鲁棒性，还另外采集了暗光条件下的连接器位姿数据1500张，总计3000组位姿数据。

训练时数据集全部采用8：2的比例将数据划分为训练集和验证集，训练集用于网络的训练，进行梯度回传网络参数更新，验证集仅仅用于网络的验证，不更新网络参数。因此，训练集有2400张图片，验证集有600张图片。为了保证数据集的多样性和代表性，训练集和验证集均为随机划分，且每个数据集均包含多种类型BTB连接器的位姿数据。
\subsection{训练方法}
除了网络结构外，网络的训练方式也是实现高精度位姿检测的关键。网络的结构是实现高精度位姿检测的基础，而网络的训练方式则是实现高精度位姿检测的关键。根据万能逼近定理，深度学习网络的表达能力是无限的，但其训练方式决定了网络的收敛速度和收敛精度。为了实现高精度位姿检测，需要对网络进行有效的训练。

在本章提出的网络中，使用了两阶段的训练方式，第一阶段使用自监督学习进行预训练，训练编码器的特征编码能力，第二阶段使用有监督学习进行微调，训练特征对比和任务回归模块。

\InsertFigure[MAE]{0.8\textwidth}{掩码自编码器预训练}{framework/MAE.png}%

编码器的训练方法借鉴了掩码自编码器（MAE）\cite{he2022masked}的训练方法，即在输入图片中随机掩码掉四分之三的图像块，然后使用编码器对掩码后的图片进行编码，最后使用解码器对全图进行重建。由于掩码图像块是随机选择的，因此网络需要学习到不同图像块之间的关联和全局的信息，从而实现对掩码图像块的重建，具体结构如\ref{fig:MAE}所示。

掩码自编码器的优点在于，在结构上，其采用了不对称的编解码结构，编码器结构层数更多，参数量更大，具有更强的特征提取能力，解码器结构层数较少，参数量小。这使得，一方面，更小的解码器参数量加速了训练过程，另一方面，更简单的解码器迫使编码器学习到更高层的特征，具有更强的特特征提取能力。在训练方法上，掩码自编码遮盖了大部分的图像块，这同样使得编码器需要学习到全局，高层的特征，强化特征提取能力，也使得输入的图像块数量大大减少，计算开销更小，加速了训练过程。在任务选择上，掩码自编码器对全局图像进行重建，因此，编码器输出的特征向量需要包含全局信息，自然包含了位姿信息，并且图像重建任务符合等变性原则，因此，掩码自编码器的训练方法符合位姿检测对特征提取能力和等变性的要求。

在微调训练阶段，使用位姿标签真值进行有监督的训练，训练较少的代数，微调编码器参数的同时，训练特征对比和任务回归模块。由于特征对比和任务回归模块的参数采用随机初始化，在训练一开始会存在较多的“脏数据”，这些数据不利于参数的更新，因此采用“warm up”策略，前期使用较小的学习率进行训练，随着训练的进行，逐渐增大学习率，最后使用较大的学习率进行训练。这样可以避免前期“脏数据”对参数更新的影响，同时也可以加速训练过程。

两阶段具体的训 数设置如表\ref{tab:params}所示。

\begin{table}[params]{预训练与微调阶段训练相关参数}{ccccc}
{参数类型 & \multicolumn{2}{c}{预训练阶段} & \multicolumn{2}{c}{微调阶段} \\
~ & 参数名称 & 参数值 & 参数名称 & 参数值}
~ & 输入图片分辨率 & $224 \times 224$ & 输入图片分辨率 & $224 \times 224$ \\
~ & 掩码比例 & $0.75$ & 掩码比例 & $0.0$ \\
~ & 掩码块大小 & $16$ & 掩码块大小 & $16$ \\
模型参数 & 归一化像素损失 & False & 归一化像素损失 & False \\
~ & 编码器尺寸 & base & 编码器尺寸 & base \\
~ & ~ & ~ & 输出特征维度 & $3$ \\
~ & ~ & ~ & 交叉注意力头数 & $12$ \\
\hline
~ & 学习率 & $10^{-3}$ & 学习率 & $10^{-3}$ \\
~ & 最小学习率 & $0.0$ & 最小学习率 & $0.0$ \\
基本参数 & 热身代数 & $40$ & 热身代数 & $10$ \\
~ & 批量大小 & $128$ & 批量大小 & $48$ \\
~ & 训练轮数 & $1200$ & 训练轮数 & $100$ \\
\hline
~ & 优化器类型 & AdamW & 优化器类型 & AdamW \\
优化器参数 & 权重衰减 & $0.05$ & 权重衰减 & $0.01$ \\
~ & 优化器因子 & $(0.9,0.95)$ & 优化器因子 & $(0.9,0.95)$ \\
\end{table}

% \begin{table}[params]{预训练与微调阶段训练相关参数}{cccccc}
% {\multicolumn{3}{c}{\textbf{预训练阶段}} & \multicolumn{3}{c}{\textbf{微调阶段}} \\
% 参数类型 & 参数名称 & 参数值 & 参数类型 & 参数名称 & 参数值}
% ~ & 输入图片分辨率 & $224 \times 224$ & ~ & 输入图片分辨率 & $224 \times 224$ \\
% ~ & 掩码比例 & $0.75$ & ~ & 掩码比例 & $0.0$ \\
% ~ & 掩码块大小 & $16$ & ~ & 掩码块大小 & $16$ \\
% 模型参数 & 归一化像素损失 & False & 模型参数 & 归一化像素损失 & False \\
% ~ & 编码器尺寸 & base & ~ & 编码器尺寸 & base \\
% ~ & ~ & ~ & ~ & 输出特征维度 & $3$ \\
% ~ & ~ & ~ & ~ & 交叉注意力头数 & $12$ \\
% \hline
% ~ & 学习率 & $10^{-3}$ & ~ & 学习率 & $10^{-3}$ \\
% ~ & 最小学习率 & $0.0$ & ~ & 最小学习率 & $0.0$ \\
% 基本参数 & 热身代数 & $40$ & 基本参数 & 热身代数 & $10$ \\
% ~ & 批量大小 & $128$ & ~ & 批量大小 & $48$ \\
% ~ & 训练轮数 & $1200$ & ~ & 训练轮数 & $100$ \\
% \hline
% ~ & 优化器类型 & AdamW & ~ & 优化器类型 & AdamW \\
% 优化器参数 & 权重衰减 & $0.05$ & 优化器参数 & 权重衰减 & $0.05$ \\
% ~ & 优化器因子 & $(0.9,0.95)$ & ~ & 优化器因子 & $(0.9,0.95)$ \\
% \end{table}
\subsection{训练结果与泛化性验证实验}
采用上述方法和数据集对网络进行两阶段训练，预训练时使用了随机水平翻转来提高数据集的多样性，使用标准归一化操作来保证训练数值的稳定性。对于预训练阶段，从两个维度考量其训练效果，一是其在验证集上的平均像素重建损失，二是其重建可视化效果。如\ref{fig:MAEPretrainLoss}所示，网络在训练过程中的平均像素重建损失逐渐减小，曲线较为平滑，在训练到1000代时基本收敛，最终收敛的平均像素重建损失为$0.0067$，说明网络的训练效果良好。\ref{fig:MAEPretrain}展示了掩码自编码器预训练阶段的重建结果，其中\ref{fig:MAEa}是原始输入，\ref{fig:MAEb}是遮挡$75\%$后网络可见的剩余图像，\ref{fig:MAEc}是将遮挡部分进行重建后的输出，\ref{fig:MAEd}是将重建部分和遮挡后的可见部分进行叠加的结果。对比原始输入和重建叠加结果可以看出，尽管输入的遮挡比例很高，掩码自编码器依然能够有效地对遮挡部分进行重建，并且很难从肉眼分辨出重建差异。因此可以说，编码器在预训练阶段较好的捕捉到了图像分块之间的关联和全局信息，对输入特征进行了有效编码。

\begin{subfigures}[MAEPretrain]{掩码自编码器预训练重建结果}
  \SubFigure[MAEa]{0.95\textwidth}{0.24\textwidth}{原始图像}{framework/MAEa.png}%
  \SubFigure[MAEb]{0.95\textwidth}{0.24\textwidth}{随机遮挡后图像}{framework/MAEb.png}%
  \SubFigure[MAEc]{0.95\textwidth}{0.24\textwidth}{重建遮挡部分}{framework/MAEc.png}%
  \SubFigure[MAEd]{0.95\textwidth}{0.24\textwidth}{重建+未遮挡图像}{framework/MAEd.png}%
\end{subfigures}

在微调训练阶段，使用有监督的标签进行训练，由于网络需要进行位姿预测，因此没有使用翻转操作进行数据增强，仅使用标准归一化操作保证数值稳定性。在验证时，主要考量其在验证集上的平滑$L_{1}$范数损失和平均绝对误差，平均绝对误差是指预测输出和标签真值在$x$、$y$、$Rz$三个位姿检测维度上的误差取绝对值之后的均值，平均绝对误差越小，说明网络的训练效果越稳定，且距离真值越近。\ref{fig:CrossMAEFinetuneLoss}和\ref{fig:CrossMAEFinetuneMAE}分别展示了微调阶段的训练损失和实际预测的平均绝对误差，其中\ref{fig:CrossMAEFinetuneLoss}是训练过程中的平均平滑$L_{1}$范数损失，\ref{fig:CrossMAEFinetuneMAEx}、\ref{fig:CrossMAEFinetuneMAEy}、\ref{fig:CrossMAEFinetuneMAERz}分别是训练过程中在三个方向上平均绝对误差。可以看出，由于编码器已经进行了充分预训练，网络在训练过程中，平均平滑$L_{1}$范数损失和平均绝对误差均快速收敛，且趋势平滑，在训练到$30$代时基本收敛，最终收敛的平滑$L_{1}$范数损失为$0.0014$，在三个预测方向的平均绝对误差分别为$0.016\,\mathrm{mm}$、$0.01\,\mathrm{mm}$、$0.04^{\circ}$，满足容差±$0.1\,\mathrm{mm}$，±$0.1\,\mathrm{mm}$，$1.5^{\circ}$\cite{yang2024accurate}。在此需要强调的是，该任务的容差是指最大误差不得超过±$0.1\,\mathrm{mm}$和±$1.5^{\circ}$，而不是平均误差。根据对误差直方图的统计，当平均绝对误差$\leq$容差的四分之一时，可以满足实际应用需求。

\begin{subfigures}[TrainLoss]{两阶段网络训练损失曲线}
  \SubFigure[MAEPretrainLoss]{0.95\textwidth}{0.45\textwidth}{预训练平均像素损失}{framework/MAEPretrainLoss.png}%
  \SubFigure[CrossMAEFinetuneLoss]{0.95\textwidth}{0.45\textwidth}{微调阶段平滑$L_{1}$范数损失}{framework/CrossMAEFinetuneLoss.png}%
\end{subfigures}

\begin{subfigures}[CrossMAEFinetuneMAE]{微调阶段三轴平均绝对误差}
  \SubFigure[CrossMAEFinetuneMAEx]{0.95\textwidth}{0.32\textwidth}{$x$轴平均绝对误差}{framework/CrossMAEFinetuneMAEx.png}% 
  \SubFigure[CrossMAEFinetuneMAEy]{0.95\textwidth}{0.32\textwidth}{$y$轴平均绝对误差}{framework/CrossMAEFinetuneMAEy.png}%
  \SubFigure[CrossMAEFinetuneMAERz]{0.95\textwidth}{0.32\textwidth}{$Rz$轴平均绝对误差}{framework/CrossMAEFinetuneMAERz.png}%
\end{subfigures}
\section{本章小结}
本章围绕手机BTB连接器的位姿检测任务，系统地提出并实现了一种数据驱动的端到端相对位姿检测网络框架。首先，阐述了在手位姿检测到相对位姿检测任务的数学建模过程，明确使用机械臂末端相对位姿作为标签的网络建模思路和从视觉输入到相对位姿估计的任务。

接着，从特征编码、特征对比、任务回归与损失计算四个环节设计了网络结构。特征编码模块基于ViT视觉变换器，兼具强特征提取能力和空间等变性；特征对比模块引入交叉注意力机制，实现两张图像特征的相互对比和相对信息提取；任务回归模块将特征向量映射为具体位姿值，并通过平滑$L_1$损失函数构建稳健的训练目标函数。

在网络训练方面，提出了结合掩码自编码器的自监督预训练与有监督微调的两阶段训练策略。预训练阶段通过遮挡图像块进行图像重建，显著增强了编码器的全局建模能力和等变性。在微调阶段，使用有监督标签训练特征对比与回归模块，进一步提升整体精度与收敛速度。

最后，基于构建的覆盖十种BTB连接器类型、不同光照条件和位姿扰动的数据集，完成网络的训练与验证实验。实验结果显示：预训练阶段的像素重建误差收敛良好，重建图像质量高，验证了编码器对输入的强建模能力；微调阶段在$x$、$y$方向和$R_z$旋转方向的平均绝对误差分别为$0.016\,\mathrm{mm}$、$0.014\,\mathrm{mm}$、$0.04^{\circ}$，远小于±$0.1\,\mathrm{mm}$和$1.5^{\circ}$的容差要求，验证了网络在高精度要求下的有效性和可行性。

综上所述，本章通过对视觉相对位姿检测任务进行定义并分析建立了一套完整的端到端基于掩码自编码器的视觉位姿检测基础框架，并给出了对应数据集构建策略和训练方法。
\chapter[modules]{位姿检测网络详细结构设计与优化}
\ref{txt:framework}已经给出了面向图像相对位姿检测任务的基于掩码自编码器的深度学习位姿检测网络的整体框架设计与训练方法，然而在实际应用中，网络的性能仍然受到多种因素的影响，例如光照条件波动、环境背景不一致、同型号不同批次连接器表面质量变化、未经过训练的新型号泛化等问题。因此，本章将对实际部署时存在的挑战进行分析，给出应对实际挑战的针对性模块设计和详细网络框架，以实现网络在实际工况下的快速部署，达到其在不同工况下的鲁棒性和较高准确性。
\section{问题分析与测试}
\subsection{问题分析}
现有的网络在测试集上表现良好，已经达到了精度要求，说明网络已经有了较强的特征编码、特征对比和任务回归的能力。但上述网络只能作为基础骨干，还需要针对在实际应用和部署中网络仍然会存在一些问题，进行更详细的模块设计和结构优化，以防止其精度在部署时下降，主要体现在以下几个方面：
\begin{itemize}
  \item \textbf{光照和成像条件变化：} 光照变化是影响网络性能的主要因素之一，尽管本文在训练时已经考虑了光照变化的影响，但实际的光照情况是不可枚举的，且光照变化会导致连接器表面反射率的变化，进而影响到特征编码和特征对比的效果。因此，还需要针对光照变化进行进一步的结构设计。
  \item \textbf{训练和部署时背景不一致：} 在数据采集时，为了训练网络对相对位姿编码和学习能力，使用了单次抓取多次位姿采集的技巧，从而减轻了数据采集的负担。但这样的采集方法也给网络的训练带来了背景不一致的问题，即在训练时，背景和连接器的相对位置固定，两者同步运动，而在实际应用时，机械臂每次都在固定的检测位姿进行位姿检测，背景和连接器存在由抓取误差产生的相对位移。这使得网络在训练时会同时编码背景和连接器的位姿特征，而在实际应用时，背景和连接器的位姿特征是分开的。因此，需要对网络进行进一步的结构设计，以避免背景和连接器的位姿特征混淆。
  \item \textbf{不同批次表面质量变化：} 在实际的工业应用场景中，同型号不同批次的零件在表面形貌和色彩上存在一定差异，这会对基于视觉传感的位姿检测方法产生影响，并且其变化同样难以估计和提前预知。因此，需要对网络进行进一步的结构设计，以增强网络对不同批次连接器的适应性。
  \item \textbf{新型号泛化：} 同一型号手机内存在多种模组与连接方式，其连接器形状、颜色、针脚数和间距等均存在差异，并且不同型号手机连接器型号连接器也都不尽相同，因此在实际应用时，网络需要具备较强的泛化能力，能够在少样本甚至零样本的条件下快速泛化的新型号连接器的位姿检测。 
\end{itemize}
\subsection{数据集扩充与测试}
为了评估所提出的基础网络框架在不同实际应用场景下的鲁棒性与泛化能力，本文对原始数据集进行了扩充。新增数据主要用于测试型在光照变化、背景干扰、表面质量变化及连接器型号差异等典型工况下的表现。

首先，为评估模型对光照变化的鲁棒性，本文额外采集了针脚间距为 $0.35\,\mathrm{mm}$、针脚数为 $24$ 的连接器图像，分别在暗光、正常光和亮光三种不同光照条件下获取各 $150$ 张图像，构建光照鲁棒性验证集。光照强度从低到高逐级变化，其采样效果如图 \ref{fig:LightIntensity} 所示，旨在模拟实际工业环境中常见的光照波动情形，检验模型在不同亮度下的性能稳定性。
\begin{subfigures}[LightIntensity]{光照强度变化示意图}
  \SubFigure[LightIntensitya]{0.9\textwidth}{0.3\textwidth}{暗光}{modules/LightIntensitya.png}%
  \SubFigure[LightIntensityb]{0.9\textwidth}{0.3\textwidth}{正常光}{modules/LightIntensityb.png}%
  \SubFigure[LightIntensityc]{0.9\textwidth}{0.3\textwidth}{亮光}{modules/LightIntensityc.png}%
\end{subfigures}

其次，为评估背景变化对模型预测精度的影响，本文模拟了实际部署中背景保持不变但目标工件姿态不同的典型情境。从原有的 $3000$ 组位姿样本中随机选取 $150$ 组，通过实例分割算法将连接器与背景进行分离，随后将不同位姿下的连接器图像与统一的末端执行器背景进行合成。此策略使得网络在保持背景一致的情况下，仅关注连接器本体的特征变化，从而进一步检验其对背景干扰的抵抗能力，操作流程及合成效果如图 \ref{fig:SegmentBG} 所示。
\InsertFigure[SegmentBG]{0.9\textwidth}{构建背景不变工况算法处理流程}{modules/SegmentBG.png}%

最后，为了验证网络对表面质量变化及不同批次零件的适应能力，本文选取了来自另一批次的连接器样本，其针脚间距同为 $0.4\,\mathrm{mm}$，针脚数为 $24$。该批次连接器在针脚表面色彩、金属光泽及形貌上与原始数据存在明显差异，如图 \ref{fig:Surface} 所示。本文采集了该批次共 $150$ 组图像，用于测试网络在应对生产过程中的表面差异、材料变化等问题时的泛化能力与鲁棒性。
\InsertFigure[Surface]{0.8\textwidth}{不同批次表面质量变化示意图}{modules/Surface.png}%

针对新补充的数据，对基础网络框架进行了训练与测试，四种工况以及在原数据集上的测试结果如\ref{tab:crossmae_error}所示。
\begin{table}[crossmae_error]{不同工况下的位姿检测误差及相对增长倍率}{lcccccc}
{数据集工况 & $x$(mm) & 增长倍率 & $y$(mm) & 增长倍率 & $R_z$(deg) & 增长倍率}
原始数据集 & 0.016 & -- & 0.014 & -- & 0.040 & -- \\
新型号泛化 & 0.036 & 1.22 ↑ & 0.020 & 0.52 ↑ & 0.28 & 5.86 ↑ \\
表面质量变化 & 0.022 & 0.39 ↑ & 0.082 & 4.98 ↑ & 0.30 & 6.52 ↑ \\
光照变化 & 0.040 & 1.43 ↑ & 0.10 & 6.39 ↑ & 0.20 & 3.91 ↑ \\
模拟部署背景 & 0.50 & 29.77 ↑ & 0.40 & 27.71 ↑ & 0.097 & 1.40 ↑ \\
\end{table}

从\ref{tab:crossmae_error}中可以看出，当前位姿检测网络在实际应用环境下的鲁棒性仍存在明显不足。尤其是在当背景保持不动、光照变化和零件表面质量变化这三类场景中，部分维度的误差增长倍率达到了$5$倍甚至近$30$倍，说明网络对背景干扰、光照波动以及连接器表面变化的适应能力较弱。其中，$y$轴方向的误差在光照和背景变化下大幅上升，$R_z$角度的估计在型号与表面变化下也受到显著影响。这表明当前模型尚未充分学习到与目标本体强相关、但对外部干扰不敏感的特征。因此，针对现有问题，进行框架优化和模块结构设计，提升网络在实际应用场景下的泛化能力和鲁棒性显得尤为必要。
\section{网络结构优化与详细结构设计}
针对实际部署时存在的问题，本节将给出三个方面的优化，分别解决实际部署时的光照变化，背景变化和表面质量变化等问题，对应的模块分别为光照对齐模块、掩码注意力模块、点云辅助对齐模块。本节将对各个模块的设计思想与实现给出详细的阐释。
\subsection{光照对齐模块}
在视觉检测任务中，光照变化是影响网络性能的主要因素之一。目前普遍的做法是通过大量采集处于不同光照条件下的数据进行训练，以提升网络对光照变化的鲁棒性。然而，由于实际场景中光照条件极其多样化，不可能穷尽所有光照组合，同时过多的数据采集也显著增加了网络的训练成本，并对模型的编码能力提出更高要求。为此，本文提出了一种轻量化的光照对齐模块，旨在从输入图像中提取光照信息并进行标准化处理，从而实现图像光照条件的统一，使网络在训练过程中能够更专注于连接器本体的特征学习，进而显著增强模型在光照变化场景下的鲁棒性。

为了更好地对光照强度进行定量分析，本文采用在固定增益条件下控制曝光时间的方式，以改变相机的进光量，构建了一套可调光照强度的数据集。具体地，通过将曝光时间从 $1ms$ 按对数增长规律增加至 $50ms$，共采集了 $14$ 组具有不同亮度级别的图像，为后续光照对齐模块的测试与评估提供实验基础。

在实现图像对齐的过程中，关键在如何准确地从图像中提取其光照强度信息，并据此对两张具有相对位姿关系的图像进行光照矫正与标准化处理。为此，本文借鉴了 Wang 等人在 2004 年提出的结构相似性评价方法 SSIM（Structural Similarity Index）\cite{wang2004image}，该方法通过分别计算图像对在亮度、对比度与结构方面的相似性，来综合衡量图像之间的视觉相似程度。其具体计算公式如\eqref{eq:SSIM}所示，其中亮度项依赖于图像的局部均值，对比度项基于局部方差，结构项则依赖于图像协方差。这一特性启发本文：图像亮度可通过全图灰度均值表示，而图像对比度则可通过灰度方差表示。

\begin{equation} \label{eq:SSIM}
\begin{aligned}
l(x, y) &= \frac{2\mu_x \mu_y + C_1}{\mu_x^2 + \mu_y^2 + C_1} \\
c(x, y) &= \frac{2\sigma_x \sigma_y + C_2}{\sigma_x^2 + \sigma_y^2 + C_2} \\
s(x, y) &= \frac{\sigma_{xy} + C_3}{\sigma_x \sigma_y + C_3} \\
\text{SSIM}(x, y) &= [l(x, y)]^\alpha \cdot [c(x, y)]^\beta \cdot [s(x, y)]^\gamma
\end{aligned}
\end{equation}

据此，本文提出的光照对齐模块从整张图像中提取全局均值与方差信息，并以线性变换的形式对输入图像进行亮度与对比度的联合矫正，从而实现图像光照条件的标准化。该模块被实现为一个可插拔的归一化处理单元，可方便地嵌入多种深度学习框架中，作为视觉前处理操作的一部分。

为评估该模块的有效性，本文在构建的数据集上对光照对齐模块进行了系统测试，并使用结构相似度（SSIM）和均方根误差（RMSE）作为定量评价指标。SSIM 越高，说明对齐后图像在亮度、对比度和结构方面与目标图像越接近；RMSE 越小，说明图像在像素值层面与目标图像的差异越小。测试结果如图~\ref{fig:LightAlign} 所示。

\begin{subfigures}[LightAlign]{光照对齐模块测试结果}
  \SubFigure[LightAligna]{0.95\textwidth}{0.45\textwidth}{对齐目标（曝光时间$50ms$）}{modules/LightAligna.png}%
  \SubFigure[LightAlignb]{0.95\textwidth}{0.45\textwidth}{原始图像（曝光时间$5ms$）}{modules/LightAlignb.png}%
  \SubFigure[LightAlignc]{0.95\textwidth}{0.45\textwidth}{均值对齐结果}{modules/LightAlignc.png}%
  \SubFigure[LightAlignd]{0.95\textwidth}{0.45\textwidth}{均值方差同时对齐结果}{modules/LightAlignd.png}%
\end{subfigures}

实验表明，在未进行任何对齐操作的情况下，输入图像的 SSIM 仅为 $0.34$，RMSE 高达 $0.298$，说明原始图像与对齐目标在光照层面存在显著差异。在仅进行均值对齐（亮度平移）后，SSIM 提升至 $0.48$，RMSE 降低至 $0.132$，表明亮度得到了初步的补偿。然而从可视化结果可以看出，图像整体偏亮且存在明显泛白现象，部分区域结构信息丢失，尤其在原始暗部区域更为严重。这是由于图像对比度与亮度存在一定正相关，仅进行亮度的均值平移会导致对比度压缩，从而降低图像层次感。

进一步地，在同时对齐图像的均值与方差（亮度与对比度）后，图像视觉效果显著改善，结构细节得以恢复，SSIM 显著提升至 $0.62$，RMSE 降低至 $0.043$，说明线性联合矫正能够有效提升图像质量，显著减少与目标图像的差异。结果表明，光照对齐模块能够有效地对输入图像进行标准化处理，从而在不依赖原始成像条件的情况下，统一光照强度，提升后续模型的输入质量。

为验证光照对齐模块在网络中的实际效果，本文将其嵌入到模型前端作为预处理步骤，并在光照变化工况下重新训练与测试网络。对比实验结果如\ref{tab:lightalign_error} 所示：

\begin{table}[lightalign_error]{光照对齐模块前后误差对比}{lcccccc}
{有无光照对齐 & $x$(mm) & 下降率(\%) & $y$(mm) & 下降率(\%) & $R_z$(deg) & 下降率(\%)}
无光照对齐 & 0.040 & - & 0.10 & - & 0.20 & - \\
有光照对齐 & 0.045 & -12.5\% ↑ & 0.068 ↓ & 32\% & 0.16 & 20\% ↓ \\
\end{table}

从表中可以看出，虽然 $x$ 轴方向的误差略有上升（上升率为 $12.5\%$），但 $y$ 轴与 $R_z$ 轴的误差分别下降了 $32.0\%$ 和 $20.0\%$，具有显著改善。$x$ 轴误差的小幅上升可能由训练过程中的随机性或数据扰动引起，不具有系统性趋势；而 $y$ 轴和平移角 $R_z$ 的大幅优化，充分说明光照对齐模块的引入有效提升了网络在光照变化环境下的鲁棒性与预测精度，验证了其在复杂工业场景中的实际应用价值。
\subsection[MaskAttentionModule]{掩码注意力模块}
从\ref{tab:crossmae_error} 中可以看出，在模拟实际部署环境中背景保持不变的条件下，网络的预测精度显著下降。其中，$x$、$y$ 和 $R_z$ 三个维度的误差分别增长了 $29.77$ 倍、$27.71$ 倍和 $1.40$ 倍，表明网络在该工况下的鲁棒性严重退化。进一步分析发现，这种性能下降主要是由于网络在训练过程中过度依赖背景区域的特征信息，未能充分提取连接器本体的结构与几何特征。当背景在测试阶段保持恒定但姿态变化时，网络无法从背景中获得有效判别特征，从而导致位姿估计失败并产生较大误差。

为解决上述问题，本文提出了一种掩码注意力模块（Mask Attention Module），旨在引导网络聚焦于连接器本体区域，从而提升其在背景变化时的特征学习能力与鲁棒性。该模块包括两个核心步骤：掩码生成与注意力增强，分别用于定位感兴趣区域和强化训练信号。

在掩码生成方面，关键在于如何准确提取连接器本体区域并构建空间掩码。本文采用基于 YOLO~\cite{redmon2016you} 的实例分割网络，并通过少量人工标注样本对其进行训练以获得稳定的分割性能。实验表明，仅使用约 150 组标注图像即可实现对连接器区域的有效识别，分割结果如\ref{fig:MaskAttentionRGBb} 所示。考虑到分割掩码可能存在覆盖不足的问题，进一步对掩码进行拓展处理。具体地，首先提取分割区域边缘并进行最小外接矩形拟合，保持主轴方向不变，并依据连接器型号对矩形框进行缩放调整，生成如\ref{fig:MaskAttentionRGBc} 所示的最终掩码。

在获得掩码之后，需构建权重图以分配区域内像素的注意力强度。本文采用由中心向外逐级衰减的权重策略：在掩码中心区域赋予较高权重，向边缘逐渐衰减，边界处权重为零。该策略的优势在于强化连接器本体中心区域的特征表达，同时抑制背景区域对网络的干扰，如\ref{fig:MaskAttentionRGBc} 中亮度所示，亮度越高表示权重越大。

\begin{subfigures}[MaskAttentionRGB]{视觉掩码权重生成策略}
  \SubFigure[MaskAttentionRGBa]{0.95\textwidth}{0.24\textwidth}{原始图像}{modules/MaskAttentionRGBa.png}%
  \SubFigure[MaskAttentionRGBb]{0.95\textwidth}{0.24\textwidth}{实例分割结果}{modules/MaskAttentionRGBb.png}%
  \SubFigure[MaskAttentionRGBc]{0.95\textwidth}{0.24\textwidth}{掩码注意力图}{modules/MaskAttentionRGBc.png}%
  \SubFigure[MaskAttentionRGBd]{0.95\textwidth}{0.24\textwidth}{使用掩码过滤背景}{modules/MaskAttentionRGBd.png}%
\end{subfigures}

在完成掩码与权重图生成后，本文分别在网络的输入端和输出端引入注意力增强机制。在输入端，原始的两张输入图像依据掩码权重图进行区域采样，仅保留权重值大于 0.5 的区域用于训练，如\ref{fig:MaskAttentionRGBd} 所示。此举可以有效剔除背景区域干扰，使网络聚焦于连接器本体区域的姿态变化，提升特征学习的针对性。

在输出端，网络首先将预测的相对位姿应用于输入的第一帧图像进行仿射变换，并与第二帧图像进行逐像素对齐。为了突出本体区域的对齐效果，本文采用加权对比损失的策略：将掩码权重图与对比误差图逐像素相乘，得到加权损失图，并以此回传优化网络参数。该策略使得损失计算更加聚焦于感兴趣区域，从而提高网络对本体区域的定位与姿态预测能力。

\InsertFigure[TransMAE-RGB]{0.9\textwidth}{掩码注意力模块的网络结构}{modules/TransMAE-RGB.png}%

基于以上设计思想，本文在原始网络结构基础上引入掩码注意力模块，构建改进模型，结构如\ref{fig:TransMAE-RGB} 所示。将该模型在背景保持不变的模拟部署工况下重新训练和测试后，获得的性能对比如\ref{tab:maskattention_error} 所示。

\begin{table}[maskattention_error]{掩码注意力模块在模拟部署情景下的误差对比}{lcccccc}
{模型方案 & $x$(mm) & 下降率 & $y$(mm) & 下降率 & $R_z$(deg) & 下降率}
基础框架 & 0.50 & -- & 0.40 & -- & 0.097 & -- \\
掩码注意力 & 0.028 & 94.4\% ↓ & 0.022 & 94.5\% ↓ & 0.055 & 43.3\% ↓ \\
\end{table}
% 需要进行一定修改 
从表中可以看出，引入掩码注意力模块后，$x$ 和 $y$ 轴方向的预测误差分别从 $0.50\,\mathrm{mm}$和 $0.40\,\mathrm{mm}$降低至 $0.028\,\mathrm{mm}$和 $0.022\,\mathrm{mm}$，下降幅度均达到 $94.5\%$ 倍，说明该模块显著增强了模型在平移方向上的定位能力。同时，$R_z$ 方向的旋转误差也由 $0.097^{\circ}$ 降低至 $0.055^{\circ}$，下降倍率为 $43.3\%$。整体结果充分验证了掩码注意力机制在背景干扰显著的场景中对提升模型鲁棒性与预测精度的有效性。
\subsection{点云辅助对齐模块}
在应对零件表面质量变化及新型号的泛化问题时，视觉图像所提供的特征往往存在较大波动，此时仅依赖视觉信息进行位姿推理较难获得稳定而准确的估计结果，并且使用视觉对齐难以学习到准确鲁棒地位姿信息。为此，本文提出在训练阶段引入视触觉传感器，生成接触区域的点云图，在损失计算使用点云对齐损失代替视觉对齐损失，通过点云对齐机制辅助位姿估计，迫使网络从视觉中提炼更具鲁棒性地特征和位姿信息。而在实际部署和使用阶段，网络依然可以仅使用视觉图像进行估计。从而实现在训练时使用触觉辅助学习更鲁棒特征，而部署阶段在不引入额外传感器的情况下，仍能获得较高的位姿估计精度和鲁棒性。

视触觉传感器因其主动接触式成像特性，在成像质量与稳定性方面相比被动视觉传感器具有显著优势。一方面，触觉图像受环境光照变化影响极小，背景信息始终保持一致，因此更容易提取出稳定的零件掩码区域；另一方面，触觉数据能反映接触部位的几何形变与局部纹理，具有更强的细节分辨能力。这些优势使得触觉图像不仅可用于生成掩码图，还可用于构建点云用于空间配准。

\subsubsection{基于视触觉信息的掩码生成}
本文提出将触觉图像替代\ref{txt:MaskAttentionModule} 中传统视觉掩码注意力模块，作为生成注意力图的主要来源。相比之下，使用触觉图像的掩码生成方式具有以下几个显著优势：

\begin{itemize}
  \item  \textbf{计算开销小、标注成本低：}传统基于视觉图像的掩码提取依赖于如 YOLO\cite{redmon2016you} 等实例分割算法，这不仅增加了额外的推理计算负担，同时需要大量准确的标注数据，制约了系统的泛化能力。
    \item \textbf{鲁棒性强：}视觉分割结果易受到外部条件影响，如光照变化、反光、遮挡等，从而导致模型在不同场景中的稳定性下降。相对而言，触觉图像生成过程环境变化小，数据输出稳定，具有更强的鲁棒性。
  \item \textbf{梯度性质优越：}实例分割输出的硬二值掩码不利于网络的梯度传播，容易导致训练不稳定。而触觉图像生成的灰度掩码可保留更多连续性特征，有助于缓解梯度不连续问题，提高模型的收敛效率和性能表现。
\end{itemize}

基于上述优势，本文提出将触觉图像用于掩码注意力图的生成，同时基于该掩码进一步采样点云，辅助实现位姿估计。相关流程如\ref{fig:MaskAttentionGray} 所示。视触觉图像由相机捕捉弹性体表面在接触过程中的形变获得\cite{sun2025tactile}。首先，通过逐像素对比按压前后的触觉图像，提取形变区域；然后取 RGB 三通道的最大差值生成灰度图，以增强区域响应的局部差异性。

\InsertFigure[MaskAttentionGray]{0.9\textwidth}{触觉图像生成掩码注意力图的过程}{modules/MaskAttentionGray.png}%

由于传感器相机成像仍可能存在微小噪声，尤其在差值图像中易产生椒盐噪声，因此引入非局部均值滤波（Non-Local Means, NLM）方法\cite{buades2011non} 进行图像去噪，以在保留边缘细节的同时抑制局部噪声，提升图像清晰度。接着，对灰度图采用自适应阈值方法生成二值图，提取候选接触区域。

为避免边缘形变带来的冗余区域干扰，本文通过连通性检测获得所有区域候选，并引入“矩形度”指标进行筛选。具体而言，对每个区域进行最小外接矩形拟合，计算其实际区域面积 $A$ 与外接矩形面积 $A_r$ 的比值 $\alpha = A / A_r$，仅保留 $\alpha > 0.5$ 的区域，剔除轮廓不规则的误识别区域。最终得到的掩码再进行尺寸与朝向调整，以匹配不同型号连接器的主轴方向和尺寸分布，得到最终的掩码图。

\subsubsection{触觉点云生成与辅助对齐机制}

触觉点云生成过程如\ref{fig:MaskAttentionCloud} 所示。为了尽可能减少背景区域的干扰，首先使用掩码图对灰度图进行筛选，仅保留连接器本体区域。随后在保留区域内进行二维随机采样，获取候选像素点。再将图像宽高归一化后映射至三维空间，其中 $x$、$y$ 分别对应图像平面坐标，$z$ 对应灰度值，从而构建出基于接触信息的三维点云。

\InsertFigure[MaskAttentionCloud]{0.9\textwidth}{触觉图像生成点云的过程}{modules/MaskAttentionCloud.png}%

在训练阶段，掩码图在输入端用于引导特征提取，仅在掩码值大于 $0.5$ 的区域内进行采样，增强模型对连接器区域特征的学习能力；在输出端，采用三维倒角距离（Chamfer Distance）作为损失函数，评估估计点云与目标点云之间的对齐误差。为了保证训练稳定性与对齐效果，首先将源点云通过三维仿射变换映射至目标坐标系，然后在相同空间下计算点云间的对齐误差。

该点云辅助机制的引入，一方面缓解了传统方法中由于视觉特征漂移导致位姿估计不准确的问题，通过连续的空间距离损失函数，有效规避了二值掩码造成的梯度断裂现象，显著提升了模型的训练稳定性；另一方面，使用触觉掩码代替视觉掩码，在减少了计算开销和标注成本的同时，提升模型的鲁棒性与泛化能力。

将触觉点云辅助对齐模块嵌入模型后，结构如\ref{fig:TransMAE-Tactile}所示。搭建完成后，对网络在表面质量变化和新型号泛化工况下重新训练与测试网络，获得的性能对比如\ref{tab:cloudassist_error_surface}和\ref{tab:cloudassist_error_surface} 所示。

\subsubsection{点云辅助对齐模块的性能评估与分析}
\InsertFigure[TransMAE-Tactile]{0.9\textwidth}{点云辅助对齐模块的网络结构}{modules/TransMAE-Tactile.png}%

如\ref{fig:TransMAE-Tactile} 所示，将触觉点云辅助对齐模块嵌入主网络后，构建了具备多模态融合能力的位姿估计框架。为验证该模块在复杂工况下的鲁棒性与泛化性能，本文设计了两组实验：一组针对零件表面质量变化场景，另一组针对新型号泛化场景。在上述两种工况下，分别对网络进行训练与测试，其性能对比如\ref{tab:cloudassist_error_surface} 和\ref{tab:cloudassist_error_new_type} 所示。

在新型号泛化工况下，点云辅助模块在 $x$ 方向误差由$ 0.036\,\mathrm{mm}$ 降至 $0.021\,\mathrm{mm}$，下降幅度达 41.7\%，表明触觉点云能有效补充视觉在横向结构变化下的几何信息，显著提升模型对新结构的感知能力；$y$ 方向误差略有上升（-4.8\%），但整体波动较小，说明该模块在纵向特征变化中的补偿作用较为稳定；在 $R_z$ 角度预测上保持一致，表明该模块不会引入姿态角估计的不确定性。

在表面质量变化工况下，该模块在 $y$ 方向和 $R_z$ 角度的误差分别下降 63.4\% 与 40.0\%，充分验证了触觉点云在处理纹理磨损、反光等视觉特征弱化情形下的补偿能力，显著提升了关键结构区域的姿态识别精度。然而，$x$ 方向误差由 $0.022 \,\mathrm{mm}$ 上升至 $0.037 \,\mathrm{mm}$（上升 68.2\%），表面上表现为精度退化，实质上反映了感知重心从背景冗余信息向连接器本体结构的聚焦转变。在原始模型中，由于零件在图像中所占区域较小，网络可能学习到背景与姿态之间的隐含偏差关系，从而在某些情况下维持较高精度；而引入点云后，模型更依赖于目标本体信息进行位姿估计，在表面质量变化显著时出现一定程度的误差回升。但总体误差仍处于可接受范围内，满足实际部署的精度要求。

\begin{table}[cloudassist_error_new_type]{点云辅助对齐模块在新型号泛化工况下的误差对比}{lcccccc}
{模型方案 & $x$(mm) & 下降率 & $y$(mm) & 下降率 & $R_z$(deg) & 下降率}
基础框架 & 0.036 & -- & 0.021 & -- & 0.28 & -- \\
点云辅助对齐 & 0.021 & 41.7\% ↓ & 0.022 & -4.8\% ↑ & 0.28 & 0.0\% \\
\end{table}

\begin{table}[cloudassist_error_surface]{点云辅助对齐模块在表面质量变化工况下的误差对比}{lcccccc}
{模型方案 & $x$(mm) & 下降率 & $y$(mm) & 下降率 & $R_z$(deg) & 下降率}
基础框架 & 0.022 & -- & 0.082 & -- & 0.30 & -- \\
点云辅助对齐 & 0.037 & -68.2\% ↑ & 0.030 & 63.4\% ↓ & 0.18 & 40.0\% ↓ \\
\end{table}

使用触觉点云辅助对齐模块辅助学习地网络在两个典型工况下均展现出良好的泛化能力与鲁棒性，尤其在视觉特征不稳定、外观变化剧烈的场景中表现更为突出。该模块不仅有效增强了模型对旋转角度与纵向偏移的估计能力，同时避免了传统视觉模块中因特征退化导致的性能下降，为实现高精度、多工况适应的位姿估计提供了一种切实可行、且不额外引入检测环节地技术路径。
\section{消融实验和网络性能对比}
本节针对\ref{fig:Network}、\ref{fig:TransMAE-RGB} 及\ref{fig:TransMAE-Tactile} 所示三种网络架构——基础框架、引入视觉掩码注意力的增强模型、以及结合触觉掩码与点云辅助对齐的多模态模型——在不同工况下的位姿估计性能进行综合评估。实验结果如\ref{tab:pose_error_change_summary} 所示，涵盖了新型号泛化、表面质量变化、光照变化以及模拟部署情景等典型工况，反映各模型的泛化能力与鲁棒性差异。

\begin{table}[pose_error_change_summary]{各模型在不同工况下相对于基础框架的误差对比与变化率}{lcccccc}
{模型方案 & $x$(mm) & 变化率 & $y$(mm) & 变化率 & $R_z$(deg) & 变化率}

% \multicolumn{7}{c}{\textbf{一般工况}} \\
% 基础框架       & 0.016 & --         & 0.014 & --         & 0.040 & --         \\
% 掩码注意力     & 0.024 & -48\% ↑    & 0.024 & -72\% ↑    & 0.10  & -150\% ↑   \\
% 点云辅助对齐   & 0.019 & -17\% ↑    & 0.016 & -18\% ↑    & 0.062 & -54\% ↑    \\

\multicolumn{7}{c}{\textbf{新型号泛化}} \\
基础框架       & 0.036 & --         & 0.021 & --         & 0.28  & --         \\
掩码注意力     & 0.027 & 23\% ↓     & 0.020 & 2.1\% ↓    & 0.51  & -84\% ↑    \\
点云辅助对齐   & 0.021 & 43\% ↓     & 0.022 & -6.6\% ↑   & 0.28  & -2.2\% ↑   \\

\multicolumn{7}{c}{\textbf{表面质量变化}} \\
基础框架       & 0.022 & --         & 0.082 & --         & 0.30  & --         \\
掩码注意力     & 0.061 & -180\% ↑   & 0.047 & 43\% ↓     & 0.29  & 3.7\% ↓    \\
点云辅助对齐   & 0.037 & -64\% ↑    & 0.030 & 64\% ↓     & 0.18  & 41\% ↓     \\

\multicolumn{7}{c}{\textbf{光照变化}} \\
基础框架       & 0.039 & --         & 0.10  & --         & 0.20  & --         \\
掩码注意力     & 0.040 & -0.8\% ↑   & 0.027 & 73\% ↓     & 0.27  & -35\% ↑    \\
点云辅助对齐   & 0.030 & 24\% ↓     & 0.019 & 81\% ↓     & 0.19  & 6.4\% ↓    \\

\multicolumn{7}{c}{\textbf{模拟部署情景}} \\
基础框架       & 0.50  & --         & 0.40  & --         & 0.097 & --         \\
掩码注意力     & 0.028 & 94\% ↓     & 0.022 & 94\% ↓     & 0.055 & 43\% ↓     \\
点云辅助对齐   & 0.013 & 97\% ↓     & 0.015 & 96\% ↓     & 0.0098 & 90\% ↓     \\

\end{table}

从表中结果可得出如下主要结论：

\begin{itemize}
  \item \textbf{增强模型普遍提升背景变化适应性：}无论是引入视觉掩码注意力，还是进一步结合触觉掩码与点云辅助对齐，两种策略在模拟部署场景下均取得了超过 94\% 的误差下降率，验证了掩码机制在抑制背景干扰方面的显著效果。其中，点云辅助模型在该场景下表现最优，$x$、$y$ 和 $R_z$ 方向误差分别下降 97\%、96\% 和 90\%，显示出更强的实际部署适应性。

  \item \textbf{视觉掩码在部分工况下表现不稳定：}在光照变化和表面质量变化场景中，视觉掩码注意力模型在 $R_z$（角度）维度出现明显误差上升，分别上升 35\% 与 3.7\%，表明其对视觉特征质量高度依赖。当视觉信息不稳定或实例分割失效时，网络难以保持稳定性能。

  \item \textbf{触觉点云策略提升视觉失效时鲁棒性：}引入触觉掩码与点云辅助对齐后，模型在所有测试工况下均未出现性能退化，尤其在表面质量变化场景中，$x$ 方向误差下降 64\%，$y$ 方向下降 64\%，$R_z$ 下降 41\%，显著优于视觉掩码模型。这表明，点云辅助对齐策略在视觉特征受限情况下学习到了更为鲁棒且通用地位姿特征，增强了网络在真实复杂环境中的泛化能力。

  \item \textbf{不同模型误差变化趋势合理且一致：}在表面质量变化场景下，引入视觉掩码和引入触觉掩码与点云辅助对齐后，均呈现 $x$ 误差上升、$y$ 和 $R_z$ 误差下降的趋势，但点云辅助模型整体精度更高，说明网络具备根据输入特征质量动态调整注意焦点的能力，符合预期设计逻辑。
\end{itemize}

综上所述，视觉掩码注意力机制可以在一定程度上缓解背景不一致对网络造成的干扰，提升其在部署场景中的稳定性。然而，该方法仍依赖于视觉特征质量，当面临光照、反光或表面损伤等情况时，其性能可能受到显著影响。相比之下，结合触觉感知的点云辅助对齐策略则在所有测试工况下均表现出更优的鲁棒性与泛化能力，充分验证了视-触觉融合学习的有效性。该方法能够在视觉特征弱化时学习到更鲁棒地位姿信息，从而构建出面向真实应用场景更可靠的位姿估计系统。


\section{本章小结}
本章围绕位姿检测网络在实际部署过程中面临的光照波动、背景不一致、批次间表面质量变化及新型号泛化等典型问题，深入分析了基础网络框架存在的鲁棒性与泛化能力不足的原因，并针对性地提出了三项结构优化策略：光照对齐模块、掩码注意力模块以及点云辅助对齐模块。

其中，光照对齐模块通过提取图像亮度与对比度的统计特征并进行联合线性归一化，有效降低了环境光照波动对模型输入特征分布的干扰；掩码注意力模块通过实例分割生成权重图，引导网络聚焦连接器本体区域，显著抑制背景特征干扰，提升了模型在部署工况下的稳定性；点云辅助对齐模块则借助视触觉传感器在训练阶段生成掩码与点云，利用三维空间对齐机制提升视觉特征变差时网络提取鲁棒特征的能力，在实际部署不引入额外开销的情况下，显著增强了模型在复杂工况中的鲁棒性与泛化能力。

通过系统的消融实验与对比分析，验证了各优化模块在不同工况下的有效性。实验结果表明，结合掩码注意力与触觉点云辅助的多模态协同训练策略，在应对复杂视觉环境时展现出优于基础模型与单一增强策略的性能表现。该方案不仅提升了位姿检测模型在背景不一致和光照波动条件下的稳定性，同时在表面质量变化和新型号泛化等严苛场景中也表现出良好的适应能力。

综上所述，本章通过针对实际部署问题所提出的模块化结构优化方法显著增强了位姿检测网络的实用性与可靠性。
\chapter[experiment]{综合实验与分析}
在\ref{txt:framework} 与 \ref{txt:modules} 中，系统性地给出了基于视觉特征提取触觉辅助训练的连接器相对位姿检测网络构建过程，详细介绍了各子模块的设计思路、信息处理流程以及多模态传感协同训练的方法，并通过纵向实验对模块级性能进行了评估和验证。各模块在精度、稳定性及适应性方面均展现出显著提升，体现了协同策略的有效性与先进性。

为进一步验证本文方法的综合性能，并从更宏观的层面体现其在实际应用中的优势与价值，本章将从以下三个方面展开深入分析：

首先，围绕位姿检测算法的检测精度与鲁棒性，开展横向对比实验。通过将本文所提出的视触觉融合检测算法与主流的视觉检测算法进行对比，验证所提方法性能优势；

其次，介绍本文搭建的自动装配实验平台，重点介绍总体软硬件架构和识别抓取与插入控制两个关键环节的系统设计，说明各模块在系统中的集成方式及其协同作用；

最后，在整体系统部署完成的基础上，进行综合插接实验，结合实验数据对所提方法的实用性与推广性展开讨论。
\section{视觉检测算法对比}
本节将对比分析本文所提出的基于视觉特征提取触觉辅助训练的连接器相对位姿检测算法与现有的基于阈值分割的传统视觉算法和基于YOLO的由粗到精两阶段算法进行横向对比，验证本文相对位姿检测算法的性能优势。
\subsection{基于阈值分割的传统视觉算法}
基于特征工程的传统视觉方法通常依赖于图像中的鲁棒性特征，如色彩、边缘等，通过视觉算法对图像进行分割与特征提取，进而结合位姿分析与匹配算法实现目标的位姿估计。

这类方法的优势在于计算开销较小、处理速度快，且在特征提取质量较高时能够达到较高的估计精度。此外，该方法不依赖大量标注数据，适用于部分数据资源受限的工业应用场景。然而，其劣势也较为明显：算法性能高度依赖于特征提取的质量，参数调节工作量大，且在光照条件变化、成像设备差异或零件表面存在污渍、划痕等情况下，特征提取稳定性显著下降，从而导致整体位姿估计鲁棒性较差。

为结合传统视觉算法的优势并提升其在复杂环境下的适应性，本文设计了一种融合深度学习与传统视觉处理的基于阈值分割的零件位姿估计算法。该算法流程如\ref{fig:TraditionalVision}所示，主要包括以下几个步骤：

\InsertFigure[TraditionalVision]{0.7\textwidth}{基于阈值分割的零件位姿估计算法流程}{experiment/TraditionalVision.png}%

\begin{itemize}
  \item \textbf{YOLO提取候选区域}：由于零件在整体图像中占比通常较小，背景复杂直接干扰传统算法处理效果，因此首先利用YOLO模型对图像进行目标检测，提取出包含零件的候选区域，从而构建结构化环境，为后续的图像处理与分割打下基础。
  
  \item \textbf{候选区域预处理}：对提取的候选区域图像，首先进行色域转换，将RGB图像转换到HSV色域，该色域通过色调（H）、饱和度（S）与明度（V）三个通道对颜色进行描述，具有更强的色彩区分能力。随后，为提升图像精度与细节分辨能力，采用像素级双三次插值算法对图像进行上采样，生成更高分辨率的图像，满足亚像素级的图像分析需求。
  
  \item \textbf{特征提取与后处理}：对预处理后的候选区域图像，采用阈值分割算法提取目标零件的显著特征区域（如中心黑色区域），并生成掩码图。考虑到成像中可能存在的阴影、反光等干扰因素，导致掩码图中出现噪点或空洞，故进一步采用形态学操作，包括开运算（去除毛刺）与闭运算（填补空洞），以获取更为规整、干净的特征区域图像。
  
  \item \textbf{边缘检测与轮廓提取}：在完成掩码图优化后，利用Canny边缘检测算法\cite{canny1986computational}提取图像中的边缘信息。若仍存在边缘不连续或噪声伪边缘等问题，通过轮廓提取与轮廓筛选步骤，提取所有封闭轮廓，并根据轮廓面积选择最大者，作为目标零件的主轮廓。

  \item \textbf{位姿估计}：在获得主轮廓后，为提取零件在图像中的主方向及位置，采用主成分分析（Principal Component Analysis, PCA）方法\cite{shlens2014tutorial}对轮廓区域内像素点进行分析。PCA是一种常用的降维方法，其基本思想是对原始数据去中心化后，计算其协方差矩阵，再求取该矩阵的特征值与特征向量。设轮廓像素点的集合为 $\{x_i = (x_i^{(1)}, x_i^{(2)})\}_{i=1}^{n}$，其样本均值为：
  \begin{equation}\label{eq:mean}
   \bar{x} = \frac{1}{n} \sum_{i=1}^n x_i
  \end{equation}
  去中心化后的样本矩阵为 $X = [x_1 - \bar{x}, x_2 - \bar{x}, \ldots, x_n - \bar{x}]^T$，协方差矩阵为：
  \begin{equation}\label{eq:cov}
   COV = \frac{1}{n} X^T X.
  \end{equation}
  对协方差矩阵 $COV$ 进行特征值分解，主方向由最大特征值对应的特征向量给出，而质心即为轮廓区域的中心坐标 $\bar{x}$。结合相机标定参数与图像到世界坐标系的映射关系，最终可得到零件在世界坐标系下的精确位姿信息。
\end{itemize}
\subsection{基于YOLO的由粗到精两阶段算法}
传统的视觉算法在处理多种零件以及多样工况时，往往难以保证特征提取过程的稳定性与准确性。为提升位姿估计在复杂场景中的适应能力，已有算法引入YOLOv7模型\cite{wang2023yolov7}，该模型通过多尺度特征融合结构和自适应锚框机制，在复杂背景下依然能够实现高效且精准的目标检测与分割，具备更强的实例分割鲁棒性和识别精度。

基于上述分析，本文设计了一种基于YOLOv7的两阶段位姿估计算法。该算法采用“由粗到精”的设计思想，借助YOLOv7强大的目标检测与特征提取能力，将复杂场景下的目标位姿估计问题划分为两个阶段：第一阶段侧重于目标的粗定位与实例分割，获取感兴趣区域并抑制干扰；第二阶段则在此基础上进一步提取精细结构特征，结合主成分分析（PCA）与线性判别分析（LDA）进行姿态推理与估计\cite{jing2023fpc}。整体算法框图如\ref{fig:YOLOVision}所示，具体流程如下所述：

\InsertFigure[YOLOVision]{0.7\textwidth}{基于YOLOv7的两阶段位姿估计算法流程}{experiment/YOLOVision.png}

\begin{itemize}
  \item \textbf{阶段一：候选区域检测与实例分割} \\
  本阶段以YOLOv7为基础网络，对输入图像进行目标检测，输出包含目标零件的候选区域（bounding box），并通过网络后处理获取实例分割掩码。该阶段主要完成从复杂背景中提取出待估计对象的关键步骤，不仅提供了明确的感兴趣区域（Region of Interest, ROI），还通过实例分割增强了目标的结构化信息，为后续分析打下基础。
  
  \item \textbf{阶段二：精细特征提取与位姿估计} \\
  在获得感兴趣区域后，进一步利用YOLOv7对ROI区域进行二次检测与分割，提取出连接器各针脚的掩码信息。将所有针脚掩码图像进行融合，生成统一的结构掩码图像。随后对每个针脚区域应用主成分分析（PCA），计算其主方向及质心坐标，并基于主轴方向构建辅助直线，将针脚划分为上下两组，实现粗粒度的针脚聚类。

  为了进一步提升位姿估计精度，引入线性判别分析（Linear Discriminant Analysis, LDA）方法进行精细姿态判别。LDA是一种监督学习中的降维与分类方法，其目标是在降维后的空间中最大化类间散度并最小化类内散度，从而提升类别可分性\cite{blei2003latent}。具体而言，设样本集合包含两个类别，类内散度矩阵为 $S_w$，类间散度矩阵为 $S_b$，则LDA的最优投影方向通过最大化下式获得：
  \begin{equation}\label{eq:lda}
  w^* = \arg\max_{w} \frac{w^T S_b w}{w^T S_w w}
  \end{equation}
  该优化问题可通过求解广义特征值问题得到。本文利用LDA对PCA提取的针脚主轴方向进行进一步分析，精确估计连接器的旋转角度及位姿状态，最终实现连接器零件的高精度位姿估计。
\end{itemize}
\subsection{对比实验与结果分析}
为了全面评估不同算法在常规工况下的位姿估计精度以及对不同型号连接器的泛化能力，本文对基于阈值分割的传统视觉算法、基于YOLOv7的两阶段位姿估计算法以及本文提出的视触觉融合算法进行了系统的对比实验。

实验中，重新采集了针脚间距为$0.4\,\mathrm{mm}$，针脚数量分别为24、30、34、40的四种型号连接器的数据。在平移范围为$[-4.5\,\mathrm{mm},\,4.5\,\mathrm{mm}]$，旋转范围为$[-7.5^{\circ},\,7.5^{\circ}]$的变动条件下，每种型号采集150组样本用于测试。视触觉融合算法的训练集包含先前采集的3000组样本，涵盖所有型号，并同时覆盖暗光与正常光两种光照条件

考虑到阈值分割算法对参数设置高度敏感，因此在24针脚型号上对算法参数进行了精细参数调优，并在该最优配置下评估其在该型号上的最高性能；随后将参数保持不变，进一步测试其在其它型号上的泛化能力。相较之下，基于YOLOv7的两阶段算法和本文提出的视触觉融合算法在所有型号上均采用统一模型进行测试，无需单独调整参数。

实验结果如\ref{fig:PoseEstimation}所示，从中可以看出以下几点：

\InsertFigure[PoseEstimation]{0.8\textwidth}{连接器位姿估计算法对比实验结果}{experiment/PoseEstimation.png}

\begin{itemize}
  \item 基于阈值分割的传统视觉算法在经过参数精调后，在24针脚型号上可获得最高精度；但在泛化至其他型号时性能显著下降。通过可视化分析发现，其在新型号上的连接器特征提取能力显著退化，且对零件表面的细小划痕和污渍十分敏感，鲁棒性较差，相关样本可视化结果如\ref{fig:TraditionalVisionResult}所示。
  
  \item 基于YOLOv7的两阶段算法依托其强大的特征提取能力和成熟的深度学习框架，在不同型号之间具有良好的泛化能力，整体精度波动较小。然而，其在平移方向上的精度略低，主要由于掩膜分割精度受限，导致后续质心计算出现偏差。
  
  \item 本文提出的视触觉融合算法在所有型号间均展现出最小的精度波动，同时在平移与旋转方向上均取得较高精度。这得益于算法中通过三种模块设计，在训练阶段学习到更为鲁棒和普适的特征，使得其在泛化过程依然保持较高的精度和鲁棒性，有效提升了对复杂工况的适应能力。
\end{itemize}
 
\begin{subfigures}[TraditionalVisionResult]{基于阈值分割的传统视觉中间过程可视化}
    \SubFigure[TraditionalVisionResulta]{0.95\textwidth}{0.281\textwidth}{最优参数分割}{experiment/TraditionalVisionResulta.png}%
    \SubFigure[TraditionalVisionResultb]{0.95\textwidth}{0.334\textwidth}{表面污渍影响}{experiment/TraditionalVisionResultb.png}%
    \SubFigure[TraditionalVisionResultc]{0.95\textwidth}{0.285\textwidth}{型号泛化}{experiment/TraditionalVisionResultc.png}%
\end{subfigures}

综上所述，本文提出的训练阶段使用触觉增强的视觉位姿检测网络，在连接器位姿估计任务中表现出更优的精度与更强的泛化能力，尤其适用于型号变化频繁及环境复杂的工业实际场景。
\section{硬件系统与数学模型}
为验证前述视觉检测算法在实际连接器插接任务中的有效性，本文构建了一套集识别抓取、位姿检测与对准、柔顺插入控制于一体的完整自动化作业系统。本节将重点说明各硬件设备的选型依据及其在系统中的具体功能，并进一步给出坐标系统定义及其标定方法。
\subsection{硬件搭建与选型}
为实现连接器从识别抓取到插接完成的自动化任务，本文设计并搭建了一套融合视觉、触觉与力控的多模态插接系统。系统任务流程主要包括上料供给、目标识别与抓取、位姿检测与对准、插接控制以及动作协调五个关键环节。根据各阶段的感知与控制需求，选用了如下核心硬件设备。

\begin{enumerate}
  \item \textbf{目标识别与定位:}连接器零件被放置在上料平台后，系统通过机械臂末端安装的Intel RealSense D435i 深度相机进行目标识别与粗定位。该相机分辨率为 $1920\times1080$，具有高帧率、标定便捷、配套算法丰富等优点，适用于快速部署与目标初步识别，尽管深度精度有限，但已能满足该环节对识别与定位的基本需求。
  \item \textbf{引导抓取:}识别定位完成后，机械臂移动至目标位置，通过 Epick 吸盘夹爪完成连接器的稳定抓取。该吸盘末端吸嘴尺寸小，适配连接器小型结构，能够在狭小空间内实现高效、低干涉的夹取操作。
  \item \textbf{位姿检测与对准:}抓取后的连接器被送至检测工位，系统采用 海康单目工业相机（MV-CS060-10GC）与Gelsight Mini 触觉传感器，分别采集视觉与触觉图像，实现高精度位姿检测与误差补偿。工业相机具有高分辨率（$3072\times2048$）和成像质量，适合提取精细结构特征；触觉传感器分辨率为 $320\times320$，成像稳定，能获取表面接触形貌，有效提升对准鲁棒性。
  \item \textbf{柔顺插接控制：}为实现稳定、受控的插接操作，在机械臂末端集成M3815 宇立力传感器，实时采集六维力/力矩数据。该传感器精度高、零漂小，为柔顺插接控制算法提供了高质量反馈。
  \item \textbf{任务协调与执行：}上述流程的动作控制由UR5e 协作机械臂执行，该机械臂重复定位精度达 $\pm0.03\,\mathrm{mm}$，支持 RTDE 实时数据接口（数据交换频率最高可达 $500\,\mathrm{Hz}$），具备强大的运动控制能力与系统集成便利性。同时其末端自带力传感器，支持笛卡尔空间下的力控模式，为柔顺装配任务提供强有力的执行保障。
\end{enumerate}

\InsertFigure[HardwareSysytem]{0.8\textwidth}{连接器插接实验系统}{experiment/HardwareSystem.png}%

本文以任务流程为主线，融合深度视觉、触觉感知与高精度力控，搭建了如\ref{fig:HardwareSysytem}所示的实验系统，实现了连接器插接任务从感知到执行的高效协同和全流程自动化。
\subsection{系统数学模型与坐标系标定}
为了实现系统中多传感器数据的高效融合与机器人精确控制，本文对各硬件设备的坐标系进行了统一定义，并建立了相应的坐标变换关系。系统整体选取机器人基坐标系作为世界坐标系，其余涉及的坐标系包括：

\begin{itemize}
  \item 机器人末端法兰坐标系（End-Flange Frame）
  \item 吸盘工具坐标系（Gripper Tool Frame）
  \item 单目相机坐标系（Monocular Camera Frame）
  \item 深度相机坐标系（Depth Camera Frame）
  \item 触觉传感器坐标系（Tactile Sensor Frame）
  \item 力传感器坐标系（Force Sensor Frame）
\end{itemize}

系统中大多数坐标系与基坐标系之间的变换关系通过机械安装过程中的几何约束进行精确保证，如吸盘工具、深度相机和触觉传感器的安装位置已在设计时考虑刚性固定关系。因此，在无需频繁更换工具的前提下，其相对变换矩阵可视为已知常量。

然而，为了实现高精度的末端力控策略，有必要对力传感器坐标系相对于机器人末端法兰坐标系的变换关系进行精确标定，进而将力传感器测得的力/力矩数据正确地转换至世界坐标系下进行控制与分析。

力传感器在出厂时已完成一次电信号与六维力/力矩数据之间的解耦标定，并提供原始的解耦矩阵。因此，本文所需进行的标定并非电信号—力数据之间的转换，而是传感器物理安装姿态与机械臂末端法兰之间的位姿关系。具体而言，目标是获得一个齐次变换矩阵 $T_{\text{flange}}^{\text{force}}$，用于将传感器坐标系中的力数据 $F_{\text{force}}$ 转换至法兰坐标系下：

\begin{equation}\label{eq:force}
F_{\text{flange}} = T_{\text{flange}}^{\text{force}} \cdot F_{\text{force}}
\end{equation}

继而借助机器人正解，可进一步转换至世界坐标系以供控制算法使用。

由于本系统的插接过程保持末端姿态基本垂直向下，故在实际应用中仅需关注力方向的姿态一致性，而无需标定坐标系原点偏移。这一简化策略有效降低了标定精度要求与操作复杂度。具体标定方法如下：为将外部力传感器测得的力/力矩数据准确映射至机器人末端法兰坐标系，本文基于双传感器数据对比与旋转拟合的方法完成了外部力传感器的姿态标定。该方法利用 UR5e 机械臂末端内置的力传感器作为参考坐标系，通过施加多组方向已知的外力，并同步采集两组三维力数据，求解两者之间的刚性旋转变换关系。标定过程主要包括以下步骤：

\begin{enumerate}
  \item \textbf{初始准备与零偏采集}：首先控制机器人移动至安全标定姿态，并在无外力作用下分别采集内置传感器与外部传感器的静态力数据，用于后续数据的零偏校正。
  \item \textbf{多方向外力施加与同步采样}：在标定姿态下，操作者依次在 $X$、$Y$、$Z$ 三个方向对末端施加外力，同时实时记录两个传感器在每个采样时刻的力输出。采集数据量设定为 500 组，保证拟合精度。
  \item \textbf{坐标系旋转矩阵估计}：每隔 100 个样本点，采用奇异值分解（SVD）算法对内外传感器的力数据进行最小均方差旋转拟合。设 $F_{\text{int}} \in R^{n\times3}$ 与 $F_{\text{ext}} \in R^{n\times3}$ 分别为内置与外部传感器的去零偏三维力采样矩阵，则两者之间满足如下关系：
  \begin{equation}
    F_{\text{int}} \approx F_{\text{ext}} \cdot R^\top
    \label{eq:force_relation}
  \end{equation}
  通过对相关矩阵
  \begin{equation}
    C = F_{\text{ext}}^\top F_{\text{int}}
    \label{eq:correlation_matrix}
  \end{equation}
  进行 SVD 分解 $C = U \Sigma V^\top$，可得最优旋转矩阵估计为：
  \begin{equation}
    R = U V^\top
    \label{eq:rotation_matrix}
  \end{equation}
  若 $\det(R) < 0$，则对 $U$ 的最后一列取负，保证 $R$ 为合法旋转矩阵。
  \item \textbf{尺度因子估计与变换评估}：为弥补两个传感器灵敏度差异，分别对每个轴计算平均力比例作为尺度因子 $s \in R^3$，并将外部传感器力数据进行变换以评估与内置数据的相关性变化。变换后的力数据为：
  \begin{equation}
    \hat{F}_{\text{ext}} = (F_{\text{ext}} \cdot R^\top) \oslash s
    \label{eq:scaled_force}
  \end{equation}
  其中 $\oslash$ 表示逐元素除法。
  \item \textbf{最终结果保存与可视化}：待采样完成后，系统将最终旋转矩阵、尺度因子及欧拉角保存至文件，并输出各轴方向变换前后的相关性曲线图，其中$Y$轴对齐前后的力曲线如\ref{fig:ForceAligned}所示，据此可以人工验证标定质量。
\end{enumerate}

\InsertFigure[ForceAligned]{0.8\textwidth}{外部力传感器标定前后$Y$轴力曲线对比}{experiment/ForceAligned.png}%

通过上述方法标定获得的旋转矩阵 $R$ 构成了从外部力传感器坐标系到末端法兰坐标系的姿态变换 $T_{\text{flange}}^{\text{force}}$，可在实际控制过程中将外部传感器力数据统一转换至世界坐标系下，提升柔顺插接的控制精度与稳定性。

此外，为更好地融合视觉与触觉信息以提升位姿检测的准确性，需要对视觉图像与触觉图像之间的空间对齐关系进行标定校准，获得平移偏执和缩放因子。

在进行视觉图像与触觉图像的对齐标定过程中，本文采用人工初始标定结合精细微调的策略，对多组图像中的连接器插槽区域进行了精准标注。具体地，分别在视觉图像与触觉图像中采用旋转矩形框的形式，标定连接器插槽的内外边界。为了提高标定的准确性与抗随机误差能力，本文共选取了100组样本图像进行标定，并在标注结果基础上完成图像间的几何对齐。

为提高标定效率与可操作性，本文自主开发了一款旋转矩形辅助标定软件，其界面如\ref{fig:CalibrationSoftware} 所示。该软件具备如下功能：

\InsertFigure[CalibrationSoftware]{0.8\textwidth}{视觉与触觉图像标定软件界面}{experiment/CalibrationSoftware.png}%

\begin{itemize}
  \item 支持批量导入多种格式图像（如 PNG, JPG, BMP 等），并按文件名自动排序；
  \item 提供基于鼠标交互的旋转矩形标注功能，用户可通过拖动与调整实现精细控制；
  \item 支持编辑模式切换，可对已有标注框进行移动、缩放与角度微调；
  \item 提供角度滑条与数值输入双调节机制，角度范围支持 $[-10^{\circ}, 10^{\circ}]$ 内任意调整；
  \item 提供标注参数（位置、宽高、角度）的直接输入与增量调整功能，便于数值精修；
  \item 自动保存每张图像的标注数据，并可导出为标准文本格式，用于后续图像配准；
  \item 支持外部 CSV 文件加载图像标注角度参考信息，便于引导性辅助操作。
\end{itemize}


在完成视觉图像与触觉图像中连接器插槽区域的旋转矩形打标后，本文进一步构建了一个基于几何变换的对齐优化模型，以实现两模态图像在像素空间上的统一对齐。

视觉图像与触觉图像中均标注有两组具有对应语义的旋转矩形，每组矩形表示连接器插槽的内外边界。为实现触觉图像在视觉图像坐标系下的对齐，定义待优化的仿射变换变量为：
\begin{equation}
\boldsymbol{p} = \left[x_{\mathrm{off}},\ y_{\mathrm{off}},\ s \right]
\label{eq:transformation_parameters}
\end{equation}
其中，$x_{\mathrm{off}}, y_{\mathrm{off}}$ 分别表示平移偏移量，$s$ 表示尺度缩放系数。变换后的触觉旋转矩形坐标定义为：
\begin{equation}
\hat{r}_t = 
\left[
s \cdot x_t + x_{\mathrm{off}},\ 
s \cdot y_t + y_{\mathrm{off}},\ 
s \cdot w_t,\ 
s \cdot h_t,\ 
\theta_t
\right]
\label{eq:transformed_rect}
\end{equation}
其中 $(x_t, y_t, w_t, h_t, \theta_t)$ 表示触觉图像中标注矩形的中心坐标、宽高与旋转角度。

为了度量视觉旋转矩形 $r_v$ 与变换后触觉旋转矩形 $\hat{r}_t$ 之间的几何误差，构建以下优化目标函数作为标定指标：
\begin{equation}
E(\boldsymbol{p}) = \sum_{i=1}^{N} 
\left[ 
\underbrace{\left\| \boldsymbol{c}_v^{(i)} - \hat{\boldsymbol{c}}_t^{(i)} \right\|}_{\text{中心误差}} 
+ \underbrace{\left\| \boldsymbol{s}_v^{(i)} - \hat{\boldsymbol{s}}_t^{(i)} \right\|}_{\text{尺寸误差}} 
+ \underbrace{w_{\theta} \cdot \Delta \theta^{(i)}}_{\text{角度误差}} 
\right]
\label{eq:alignment_loss}
\end{equation}

其中：
\begin{itemize}
  \item $c$ 表示矩形中心坐标；
  \item $s$ 表示矩形宽高尺寸；
  \item $\Delta \theta^{(i)}$ 为第 $i$ 组矩形的角度误差（考虑角度周期性）；
  \item $w_\theta$ 为角度误差项的权重；
  \item $N$ 为参与优化的数据样本数量。
\end{itemize}

为求解上述最小化问题，本文采用带边界约束的L-BFGS-B\cite{zhu1997algorithm}迭代移优化算法。该算法在设定初始平量与缩放系数的基础上，搜索最优参数 $\boldsymbol{p}^*$ 使得目标误差函数 $E(\boldsymbol{p})$ 最小。最终获得的最优参数可用于将任意触觉图像仿射变换到视觉图像坐标系下，实现触觉图像的空间对齐。对齐结果示例如\ref{fig:AlignedExample} 所示。

\InsertFigure[AlignedExample]{0.8\textwidth}{视觉与触觉图像对齐示例}{experiment/AlignedExample.png}%

从对齐结果看出，视触觉对齐重叠度很高，效果良好。该方法结合人工打标与带边界约束参数优化方法，实现了高效、高精度视触觉对齐。

\section{软件系统框架}
在上一节介绍硬件系统及其数学模型的基础上，本节将对系统的软件框架进行详细阐述。从任务流程角度看，软件框架主要包括三个核心模块：识别定位与抓取、位姿检测与对准、柔顺插入控制。其中，位姿检测与对准算法已在\ref{txt:framework}和\ref{txt:modules}中进行了详细介绍，故本节不再赘述。本文将从系统集成的角度重点介绍算法的整体框架，并详述识别定位与抓取模块以及柔顺插入控制模块的具体设计与实现。
\subsection{总体框架设计}
从操作流程的角度，本文的软件系统可划分为三个核心模块，其整体架构与执行步骤如\ref{fig:SoftwareFramework}所示。

\InsertFigure[SoftwareFramework]{0.6\textwidth}{软件系统框架设计}{experiment/SoftwareFramework.png}%

首先，识别与抓取模块负责连接器的初步识别与定位。该模块基于 YOLO 框架，对采集到的 RGB 图像进行目标检测，从而识别连接器的类别及其在图位姿，并进行抓取顺序规划。根据检测结果，机械臂完成运动规划并移动至目标位置，随后利用吸盘夹爪完成连接器的抓取操作。

位姿检测与对准模块的原理与方法在前述章节中已有详细阐述，此处不再赘述。需要特别强调的是，在系统实际部署过程中，无需采集触觉图像，触觉图像仅在网络训练阶段用于监督学习与对齐辅助，从而提升模型对连接器关键视觉特征的关注度与鲁棒性。在部署阶段，系统仅使用当前视觉图像与标准模板进行特征提取与比对，通过深度神经网络输出相对位姿估计结果，并据此对当前机械臂末端位姿进行修正。

柔顺插入与控制模块则用于执行插入任务中的力控制与轨迹修正，以避免因插入力过大或插入方向偏差而导致连接器卡阻或损坏。在完成姿态修正后，系统对力传感器进行初始化并消除零点偏置，然后根据目标位置规划最小加加速度（Minimum Jerk）插入轨迹，生成平滑的期望路径。在插入过程中，系统实时采集力/力矩信息，并依据预设的柔顺控制策略动态调整机械臂的运动轨迹。同时，采用 PD 控制器将控制位置映射为控制轨迹，从而提升系统的响应速度与控制稳定性，确保插入操作顺利完成。

从软件架构角度，本文的软件系统采用ROS作为底层通信与调度平台，借助话题和服务机制实现软件和硬件的高效通信与协同。软件框图如\ref{fig:ROSFramework}所示，可划分为硬件层、控制层、中间层和算法层。

\InsertFigure[ROSFramework]{1.0\textwidth}{基于ROS的软件框架设计}{experiment/ROSFramework.png}%

其中硬件层为各类传感器和执行器提供底层驱动与接口。控制层负责与硬件层进行数据交换与指令下发，对常见的数据采集、运动控制等功能进行封装。中间层作为各模块之间的桥梁负责协同各个模块，实现数据流的传递与处理，完成自动插接任务的整体协调。算法层则集成了本文的核心算法，包括识别定位与抓取算法，位姿检测与对准算法，以及柔顺插入控制算法等核心功能模块。
\subsection{识别抓取算法}
受限于机器人系统的绝对定位误差和标定精度限制，在识别与抓取阶段，机械臂在定位连接器时通常存在毫米级的位置偏差，难以实现精确的位姿估计。于是，本文设计并实现了一种基于YOLOv7目标检测算法的识别与抓取策略，实现了对连接器零件的快速识别与定位，并根据检测结果规划机械臂的抓取动作，完成了适用于批量多种类零件的毫米级误差的识别定位和鲁棒抓取任务。其核心算法和实现流程如\ref{fig:GrabModule}所示。

\InsertFigure[GrabModule]{0.7\textwidth}{识别抓取算法模块流程图}{experiment/GrabModule.png}%

在识别定位和抓取模块中，系统采用安装于机器人末端的 RealSense D435i 深度相机对位于料台上的连接器进行图像采集。随后，将获取的RGB图像随后被输入至预训练的 YOLOv7 网络模型中，进行多类别连接器及其背板区域的快速识别与检测。通过使用主成分分析算法，对计算每个连接器背板掩码的质心和主轴方向，从而获得在图位姿的粗略估计。为了将该信息转化至机器人可执行的空间坐标，系统进一步结合相机内参与外参矩阵，完成图像坐标系向机器人末端法兰坐标系的转换，获取各连接器相对于末端执行器的三维位姿。

在获取所有连接器的相对位姿后，系统规划不同零件的抓取顺序给出抓取优先级，以提升整体操作效率并避免机械干涉。本文采用先归类再排序的算法，首先依据连接器背板质心在 $y$ 轴方向进行 k-means 聚类，采用最小距离分配与质心更新的迭代策略\cite{ikotun2023k}，将连接器零件分为三组。随后，按照各组质心 $y$ 坐标从大到小进行组间排序，并在每组内根据 $x$ 坐标从小到大进行组内排序。结果如图\ref{fig:GrabModule}所示，排序结果作为模块最终输出，为后续抓取动作提供依据。
\subsection{柔顺插入控制算法}
为保证连接器插入过程安全可控，防止损坏连接器或插槽，在插接过程需要使用柔顺插入控制算法，使得插入过程中垂直插入方向的力尽量小，沿插入方向力不超过极限阈值。为此，本文设计一种基于导纳控制\cite{calanca2015review}的笛卡尔工作空间下的柔顺插入控制算法。控制外环采用导纳控制对插入轨迹进行修正，内环采用PD控制，提高机械臂的追踪和响应速度，提高控制稳定性。算法控制方框架如\ref{fig:AdmittanceControl}所示。

\InsertFigure[AdmittanceControl]{0.7\textwidth}{柔顺插入控制算法模块流程图}{experiment/AdmittanceControl.png}%


带有参考目标的导纳控制的核心思想在于将机械臂系统看作由质量、阻尼和弹簧组成的二阶系统。当机器人在向目标位置移动时，其受到的外力和产生的位置扰动和外力满足式\eqref{eq:admittance_control}所示二阶特性。

\begin{equation} \label{eq:admittance_control}
M_d \ddot{e} + D_d \dot{e} + K_d e = F_{\text{ext}}
\end{equation}

其中$M_d$、$D_d$、$K_d$分别代表系统期望质量、阻尼和刚度，通常由开发者根据插接任务对系统柔顺性能的需求进行参数设定，$e = x_d - x_0$是修正位置和初始期望的差，代表退让位移。将式\eqref{eq:admittance_control}作为先验约束，其是开发者的美好幻想与期望特性。除此之外，机械臂还必须满足动力学规律，在笛卡尔空间坐标系下，其动力学关系如式\eqref{eq:admittance_control_dynamics}所示。

\begin{equation} \label{eq:admittance_control_dynamics}
M(x) \ddot{x} + C(\dot{x},x) \dot{x} + g (x) =F_{\text{cmd}} + F_{\text{ext}}
\end{equation}

其中$M(x)$、$C(\dot{x},x)$、$g (x)$分别代表机器人当前构型下的惯性矩阵、科里奥利矩阵和重力效应，其受机器人当前构型和速度影响。将式\eqref{eq:admittance_control}带入机器人的动力学方程中，从而获得其在笛卡尔空间下的修正轨迹。因此，由于约束了退让位移和外力的关系，机械臂在插入过程中会根据外力作出位置修正，对外表现出柔顺性\cite{ott2010unified}。

但导纳控制器的控制目标是最小化系统外力，使用轨迹修正表现出柔顺性，因此导纳控制器输出的是修正后的位置。如果直接使用位置控制进行轨迹跟踪，会使得由于位置环的滞后性使得控制系统不稳定，为此，外环导纳的基础上，设计并实现了内环的PD控制器，将控制位置映射为控制力，从而使用内环力控来提高系统的响应速度和控制稳定性。PD控制器的关系如式\eqref{eq:pd_control}所示。

\begin{equation} \label{eq:pd_control}
F_{\text{cmd}} = K_p (x_{d} - x) + K_d (\dot{x}_{d} - \dot{x})
\end{equation}

除了底层控制器的柔顺控制外，在上层规划上，为保证插接过程轨迹的平滑性与连续性，本文采用最小加加速度方法\cite{viviani1995minimum}对末端执行器的插入路径进行规划。该方法通过最小化轨迹的三阶导数，避免因加速度突变造成的系统震动。

在笛卡尔空间中，对于任意一维的位置插值，其表达式如下：

\begin{equation}
p(t) = p_0 + (p_f - p_0) \left[ 10\left( \frac{t}{T} \right)^3 - 15\left( \frac{t}{T} \right)^4 + 6\left( \frac{t}{T} \right)^5 \right]
\end{equation}

其中，$p_0$ 和 $p_f$ 分别为初始位置和目标位置，$T$ 为插入动作的总时长，$t \in [0, T]$ 为当前时间。该公式在 $t=0$ 和 $t=T$ 时具有零速度和零加速度边界条件，保证了轨迹的光滑性。

本文同时对姿态部分进行最小加加速度插值，采用轴角表示方式对旋转角度进行平滑插值，确保末端在空间位置和方向上的整体协调性与一致性。

该控制策略的显著优势在于，从底层控制器上，机器人能够根据外部扰动力实时调整其末端运动轨迹，在物理层面上表现出一定的“顺从性”，有效缓解插入阶段由于定位误差、插孔公差或装配偏差引起的卡滞与冲击现象，显著提升装配过程的稳定性与安全性。从上层规划器上，使用最小加加速度规划，减少了机器人运动过程中的震动和冲击，进一步提升了装配精度与可靠性。
\section{实验效果与分析}
在完成识别抓取、位姿检测与对准、柔顺插入控制等核心模块的设计与实现后，本文在实际机器人系统上开展了多型号全流程批量插接实验，从成功率和插接力等多个维度对系统性能进行综合评估。

实验对象形貌及其种类如\ref{fig:ExperimentObjects}所示，共覆盖了针脚间距$0.35\,\mathrm{mm}$和$0.4\,\mathrm{mm}$两种，针脚数为24、30、34、40、60的共9种连接器型号。

\InsertFigure[ExperimentObjects]{1.0\textwidth}{实验对象示例}{experiment/ExperimentObjects.png}%

每次实验开始时，连接器放置在\ref{fig:HardwareSysytem}所示的料台上，系统通过末端深度相机进行拍照完成所有零件的一次性识别与定位，并给出抓取优先级，随后机械臂以此完成抓取动作。抓取完成后，机械臂移动至检测工位进行相对位姿检测与对准。并在插入力控阶段，使用柔顺插入控制算法对连接器进行插入操作，其实际流程如\ref{fig:ExperimentProcess}所示。

\InsertFigure[ExperimentProcess]{1.0\textwidth}{实际插接流程示意图}{experiment/ExperimentProcessNormal.png}%

针对料台上的 9 种连接器，本文共进行 50 轮次批量插接实验，总计完成 450 次插接任务。实验结果表明，本文提出的方法具有高度鲁棒性和稳定的精度，在所有插接任务中，没有出现插接失败情况，所有种类的连接器均能顺利插接到位，插接成功率高达$100\%$。

进一步对插接过程中的插入力与插接力矩进行了实时监测与统计分析，结果如\ref{fig:ExperimentForce}所示。从中可以看出一下几点：

\InsertFigure[ExperimentForce]{1.0\textwidth}{插接过程中的插接力和插接力矩}{experiment/ExperimentForce.png}%

\begin{itemize}
  \item 在垂直插入方向上，系统所施加的插入力始终维持在较低水平，最大值未超过 $0.5\,\text{N}$，表明位姿检测环节具有较高的对准精度，且柔顺插入控制能够有效缓解位置偏差带来的作用力。
  \item 沿着插入方向，插入力较高，峰值超过$15N$。这是由于连接器在设计时为了保证最终接触稳定，存在卡扣设计，插入过程中需要克服卡扣的摩擦力和弹性力，导致插入方向的插入力较大。从插入完成后插入力快速归零的情况看，也能说明插入过程平稳，能够克服卡阻完成柔顺插入。
  \item 插接力矩的整体波动幅度较小，在绕$y$轴和绕$z$轴方向的力矩几乎为$0$，这说明位姿检测在$x$方向平移和旋转方向上预测精度较高，插入过程没有出现偏载。在绕$x$方向存在力矩变化，但整体保持在$0.1Nm$以下。绕$x$轴和绕$y$轴的力矩差异是由于连接器本身沿着$x$方向的尺寸较大，$y$方向的尺寸较小，使得其在位姿检测精度一致的情况下，$y$方向的相对误差更大，从而产生绕$x$轴的偏载，但其整体力矩不超过$0.1Nm$，同样保证了插接过程平稳和较高的插接质量。
\end{itemize}

综上所述，本文提出的自动插接系统能够较好的适应实际插接工况，具备对多种类、大批量零件的鲁棒性和通用性，系统在整个插接过程中表现出高度的稳定性与一致性，能够有效保障插接质量，满足复杂装配任务中对精度、效率与可靠性的综合要求，具有较强的实际应用价值。
\section{本章小结}
本章围绕本文提出的手机BTB连接器自动插接系统，从软件算法对比、系统集成和综合实验验证三个方面，系统性地验证了本文方法的有效性和实用性。

首先，通过使用将已有算法和本文提出相对位姿检测算法进行定量对比实验，证明了本文提出算法在精度、鲁棒性及模型泛化能力方面的优势，特别是在多型号连接器和复杂工况下展现出更优的性能表现。这充分证明了本文使用触觉图像增强训练，有效提高了视觉检测网络特征提取的鲁棒性和优越性。

其次，详尽地介绍了自动化插接系统地硬件平台和软件架构设计，涵盖从硬件选型、坐标标定、任务流程控制及多模块协同机制，构建了集感知、决策与执行于一体的完整系统架构。

最后，通过在实际机器人平台上开展的大规模多型号插接实验，全面验证了系统在识别抓取、姿态对准和柔顺插入各环节的稳定性和有效性。实验结果表明，本文所设计的系统在插接成功率、插入力控制和力矩稳定性等关键指标上均表现出色，插接成功率达到 $100\%$，具备良好的系统鲁棒性与工程适应性。

综上所述，本章的研究与分析不仅全面验证了各核心模块在工程实践中的可行性与有效性，也进一步彰显了所提方法在实际工业装配场景中的应用潜力与推广价值。
\chapter{总结与展望}
\section{论文工作总结}
本文面向手机装备流水线中手机BTB连接器的自动插接问题，旨在针对多种类、大批量、小尺寸、复杂形貌的连接器对象，设计并实现一套从识别定位与抓取、位姿检测与对准到柔顺插入控制的完整自动化插接系统。并针对核心的位姿检测算法，分析并克服其实际插接环境位姿检测任务中光照波动、多型号泛化、背景工况不一致和不同批次表面质量变化等问题带来的挑战，给出一套兼具高精度、高鲁棒、强泛化能力的相对位姿检测算法。

本文的主要研究工作及成果如下：
\begin{enumerate}
  \item 通过阅读已有研究和文献，总结了目前视觉检测领域已有的研究方法和技术发展脉络，分析了现有方法在手机BTB连接器自动插接任务中的局限性和不足之处，并选定了基于深度学习方法解决位姿检测任务的核心技术路线。
  \item 针对手机BTB连接器的位姿检测问题，明确了其数学定义，论证了通过相对位姿检测进行在手位姿检测的理论依据。并从数据驱动的角度出发，深度剖析了从视觉输入到相对位姿输出的中间过程和处理逻辑。并基于等变性的思想，给出了模块选择方法并设计了网络基础框架，提出了基于掩码自编码和交叉注意力的相对位姿检测算法。最终，通过实验验证了该方法在数据集上的有效性，证明了该方法在手机BTB连接器的位姿检测任务中的可行性。
  \item 针对手机BTB连接器在实际应用部署中光照波动、多型号泛化、背景工况不一致和不同批次表面质量变化等挑战，针对性的提出了三种模块增强方法。通过光照对齐模块，将输入数据进行标准化处理，提升了光照条件的一致性。使用掩码生成模块，提高了模型对连接器本体的关注度，克服了训练与部署背景不一致的问题。在训练阶段引入触觉传感器，通过触觉生成点云增强训练，提高了网络对鲁棒视觉信息的提取能力，克服了不同批次表面质量变化和新型号泛化时视觉特征变化的问题。最终，给出了一套能克服实际部署挑战，兼具高精度、高鲁棒性和强泛化能力的相对位姿检测算法。并通过纵向消融实验验证了各个模块增强的有效性，横向对比实验验证了本文算法在精度、鲁棒性和泛化能力方面的优越性。
  \item 设计并实现了识别抓取和柔顺插入控制模块，使用ROS对系统进行集成，构建了完整的手机BTB连接器自动插接系统，实现了插接任务的全流程自动化。并通过综合插接实验，验证了本系统在实际应用中的有效性和实用性，达到了实验范围内的$100\%$成功率和稳定的插接质量，证明了本文方法在实际工业装配场景中的应用潜力。
\end{enumerate}
\section{未来展望}
通过总结本论文的研究工作和成果，本文在手机BTB连接器自动插接领域取得了一定的进展，但仍存在一些不足之处和未来改进的空间。以下是对未来工作的展望：
\begin{enumerate}
  \item \textbf{在位连接器的插接：}本文对于连接器插接给出了一套基于检测的解决方案，但在实际应用中，部分连接器处于在位状态，即连接器所连模块已经安装在手机上，无法进行通过视觉进行位姿检测。此时需要考虑如何使用其它感知手段，调整连接器位姿，完成插接任务，例如使用力传感器，通过力觉伺服，根据插接过程的力反馈在不进行视觉检测的条件下完成在位插接。
  \item \textbf{小样本新型号的快速适应：} 实际生产中连接器型号频繁更替，要求系统具备快速适应新型号的能力。未来可结合小样本学习、迁移学习或生成对抗网络（GAN）等方法，在有限标注样本下高效完成模型微调与泛化，从而提升系统在实际部署中的灵活性与实用性，减少模型重新训练的时间成本。
  \item \textbf{算法泛化能力的进一步提升：} 尽管本文所提出的相对位姿检测算法在多型号连接器上表现出良好的性能，但对于形貌差异显著、表面质量不一致或未见过的新型号连接器，仍可能出现识别与估计精度下降的现象。未来可从模型结构与数据增强策略两方面进行拓展，例如引入自适应特征提取模块、多尺度感知机制或多模态自监督学习框架，增强模型对关键区域的感知能力，从而提升其对复杂工况与新型号的适应能力。
  \item \textbf{复杂装配约束条件下的插接任务扩展：} 当前研究主要集中在标准插接操作场景下，后续可面向更复杂的工况，如插接角度受限、装配空间狭窄、柔性连接器插接等，设计更加灵活的运动规划与柔顺控制策略，拓展系统在非结构化工业环境中的适用范围。
\end{enumerate}
\bibliography{references}%声明并放置参考文献

\StartAppendix% 附录开始
% \chapter{书面翻译}
% \begin{center}
% 通过自适应刚度的导纳控制组装低刚度部件
% \end{center}

% 写出至少 5000 外文印刷字符的调研阅读报告或者书面翻译 1-2 篇（不少于2 万外文印刷符）。

% 这是附录中的插图示例，
% \InsertFigure[appdxfig]{}{附录中的插图示例}{sample.png}%
% 附录中的图片编码前冠以附录的序号，例如“\ref{fig:appdxfig}”。表格及公式亦如是，这里不再赘述。

% \begin{center}
% 参考文献（或书面翻译对应的原文索引）
% \end{center}
% \begin{reflist}
%   \item 调研报告中的参考文献，请自行编号。
% \end{reflist}

% \chapter[require]{清华大学综合论文训练写作规范（试行）}
% 在综合论文训练阶段，本科生须在指导教师的指导下针对某一课题进行探讨、分析和研究，并完成一篇论文。学生应遵照本规范的具体要求进行撰写。原则上，本科生（含国外来华留学本科生）非外语专业论文统一要求用汉语书写。

% \section{论文组成部分及顺序}
% 论文应包含以下部分，顺序如下：
% \begin{itemize}
%   \item 封面
%   \item 关于论文使用授权的说明
%   \item 摘要
%   \item Abstract
%   \item 目录
%   \item 插图和附表清单（如有）
%   \item 符号和缩略语说明（如有）
%   \item 正文：第1章（引言或绪论），第2章，……，结论
%   \item 参考文献
%   \item 附录
%   \item 致谢
%   \item 声明
%   \item 在学期间参加课题的研究成果（如有）
%   \item 综合论文训练记录表
% \end{itemize}
% 以上各项均独立成为一部分，每部分从新的一页开始。

% \section{论文格式要求}

% \noindent\textbf{（一）封面}

% \textbf{题目：}论文题目严格控制在25个汉字（符）以内。字体采用黑体一号字，居中书写。一行写不下时可分两行写，并采用1.25倍行距，断行应合理，应保持术语和词语连续。

% \textbf{系别：}院（系）名称的全称。

% \textbf{专业：}以本年级《学生手册》中的清华大学本科专业设置为准。

% \textbf{姓名：}填写论文作者姓名。

% \textbf{指导教师：}填写指导教师姓名，后衬指导教师专业技术职务，如“教授”、“研究员”等，副指导教师、联合指导教师与此相同。

% 系别、专业、姓名及指导教师信息部分使用仿宋三号字。若不超过4个汉字，作者姓名和指导教师姓名应等宽，各自应保持均匀间隔。

% \textbf{论文成文打印日期：}填写论文成文打印的日期，用宋体三号字，不用阿拉伯数字。

% \noindent\textbf{（二）关于论文使用授权的说明}

% 单设一页，排在封面后。内容为“本人完全了解清华大学有关保留、使用综合论文训练论文的规定，即：学校有权保留论文的复印件，允许论文被查阅和借阅；学校可以公布论文的全部或部分内容，可以采用影印、缩印或其他复制手段保存论文。”

% 此部分内容可以直接下载《清华大学综合论文训练写作规范（试行）》附件的WORD 文档，相应地复制到论文中即可，在提交论文时作者和指导教师都必须签署姓名。

% \noindent\textbf{（三）摘要}

% \noindent 1. 中文摘要

% 此部分单设一页。标题为“摘要”，用黑体三号字，居中书写，段前空24磅，段后空18磅，单倍行距。内容部分采用宋体小四号字，两端对齐，行距用固定值20 磅，段前后0磅。

% 论文摘要中不要出现图片、图表、表格或其他插图材料。

% 关键词是为了文献标引工作、用以表示全文主要内容信息的单词或术语。关键词3~5个，另起一行，排在摘要的左下方，每个关键词之间用分号间隔。

% \noindent 2. 英文摘要

% 中文摘要页后为英文摘要，单设一页。标题为“Abstract”，用Arial体三号字，居中书写，段前空24磅，段后空18磅，单倍行距。内容采用Times New Roman体小四号，两端对齐，行距用固定值20磅，段前后0磅，标点符号用英文标点符号。“Keywords”与中文摘要部分的关键词对应，每个关键词之间用分号间隔。

% 论文摘要的中文版与英文版文字内容要对应。

% \noindent\textbf{（四）目录}

% 目录是论文各组成部分章、节序号和标题行以及页码按顺序的排列，列至二级节标题（例如1.2.5）即可。目录内容从正文部分开始，直至论文结束。

% 目录的标题用黑体三号字，居中书写，单倍行距，段前空24磅，段后空18磅。目录中的章标题行居左书写，一级节标题行缩进1个汉字符，二级节标题行缩进2个汉字符。目录中的章标题采用小四号字，中文采用黑体，英文和数字采用Arial体。其他内容采用宋体小四号字，行距为固定值20磅，段前、段后均为0磅，英文和数字用Times New Roman体。章标题和节标题要简洁，尽可能保持在一行内，若确有必要超过一行，采用悬挂对齐的方式。

% 目录宜在文档编辑软件中自动生成，并根据上述要求调整格式。

% \noindent\textbf{（五）插图和附表清单}

% 论文中插图和附表较多时，应分别列出“插图清单”和“附表清单”。插图清单在前，应列出图序、图题和页码。附表清单在后，应列出表序、表题和页码。

% 插图较多而附表较少、或者插图较少而附表较多、或者二者均较少时，可将插图和附表合在一起列出“插图和附表清单”，插图在前、附表在后。

% 插图和附表清单另起一页，置于目录之后。

% 章标题“插图清单”“附表清单”或“插图和附表清单”使用黑体三号字，居中，段前空24磅，段后空18磅，单倍行距。内容部分中文采用宋体小四号字，英文和数字采用Times New Roman体小四号，行距为固定值20磅，段前、段后均为0磅。图表标题应简洁，尽可能保持在一行内，若确有必要超过一行，采用悬挂对齐的方式。图中的分图无需在图表清单中体现。

% 插图与附表清单宜在文档编辑软件中自动生成，并根据上述要求调整格式。

% \noindent\textbf{（六）符号和缩略语说明}

% 如果论文中使用了大量的物理量符号、标志、缩略词、专门计量单位、自定义名词和术语等，应编写“符号和缩略语说明 ”。如果符号和缩略词使用数量不多，可以不设专门的“符号和缩略语说明”，而在论文中出现时随即加以说明。

% 章标题“符号和缩略语说明”使用黑体三号字，居中书写，单倍行距，段前空24 磅，段后空18磅。内容部分采用宋体小四号字，行距为固定值20磅，段前、段后均为0磅。英文和数字用Times New Roman体。

% \noindent\textbf{（七）正文}

% \noindent 1. 一般要求

% 此部分是论文的主体，应从另页右页开始，每一章应另起页。主体部分一般从引言（绪论）开始，以结论结束，分章节论述，层次分明、逻辑性强。

% \noindent 2. 标题格式

% \textbf{各章标题，例如：“第1章 引言”}

% 章序号采用阿拉伯数字，章序号与标题名之间空一个汉字符。采用三号字，居中书写，中文采用黑体，英文和数字采用Arial体，单倍行距，段前空24磅，段后空 18 磅。论文的摘要、目录、插图和附表清单、符号和缩略语说明、参考文献、附录、致谢、声明、综合论文训练记录表等部分的标题与章标题属于同一等级，也使用上述格式；英文摘要部分的标题“Abstract”采用Arial体三号字。

% \textbf{一级节标题，例如：“2.1 实验装置与实验方法”}

% 节标题序号与标题名之间空一个汉字符（下同）。采用四号（14pt）字居左书写，中文采用黑体，英文和数字采用Arial体，行距为固定值20磅，段前空24磅，段后空6磅。

% \textbf{二级节标题，例如：“2.1.1 实验装置”}

% 采用13pt字居左书写，中文采用黑体，英文和数字采用Arial体，行距为固定值20磅，段前空12磅，段后空6磅。

% \textbf{三级节标题，例如：“2.1.2.1 归纳法”}

% 采用小四号（12pt）字居左书写，中文采用黑体，英文和数字采用 Arial 体，行距为固定值20磅，段前空12磅，段后空6磅。一般情况下不建议使用三级节标题。

% \noindent 3. 论文段落的文字部分

% 采用小四号（12pt）字，汉字用宋体，英文用Times New Roman体，两端对齐书写，段落首行左缩进2个汉字符。行距为固定值20磅（段落中有数学表达式时，可根据表达需要设置该段的行距），段前空0磅，段后空0磅。
% 上述论文段落文字格式亦适用于正文后附录、致谢等部分的段落文字。

% \noindent 4. 注释脚注

% 当论文中的字、词或短语，需要进一步加以说明，而又没有具体的文献来源时，用注释，采用文中编号加“脚注”的方式。在正文中需要注释的句子结尾处用%
% \begingroup\setmainfont{SimSun}\textsuperscript{①②③}\endgroup
% ……样式的数字编排序号，以“上标”字体标示在需要注释的句子末尾。在当页下部书写脚注内容。

% 脚注内容采用小五号字，中文用宋体，英文和数字用Times New Roman体，两端对齐格式，段前后均空0磅，单倍行距，悬挂缩进1.5字符。脚注的序号按页编排，不同页的脚注序号无须连续。

% 论文中应注意区分各种字符的正斜体、黑白体、大小写、上下角标、上下偏差等。

% \clearpage\noindent 5. 字体、字型、字号及段落格式要求表
% \begin{table}[format1]
% {字体、字型、字号及段落格式要求表}{p{3em}p{4.5em}p{5em}p{6.5em}p{14.5em}}
% {& \textbf{文字举例} & \textbf{中文字体、\newline 字号要求} & \textbf{英文及数字字体、字号要求} & \textbf{其他格式要求}}
% \textbf{章标题} & 第1章 & 黑体三号字 & Arial三号 & 居中书写，单倍行距，段前空24磅，段后空18磅 \\
% \textbf{一级节标题} & 4.1 实验方法 & 黑体四号字 & Arial体14pt & 居左书写，行距为固定值20磅，段前空24磅，段后空6磅 \\
% \textbf{二级节标题} & 3.2.2 实验 装置 & 黑体13pt & Arial体13pt & 居左书写，行距为固定值20磅，段前空12磅，段后空6磅 \\
% \textbf{三级节标题} & 5.3.3.2 原\newline 材料 & 黑体小四号字 & Arial体12pt & 居左书写，行距为固定值20磅，段前空12磅，段后空6磅。 \\
% \textbf{正文} & 实验预期效果 & 宋体小四字 & Times New Roman 12pt & 两端对齐书写，段落首行左缩进2个汉字符。行距为固定值20磅（段落中有数学表达式时，可根据表达需要设置该段的行距），段前空0磅，段后空0磅 \\
% \textbf{图题} & 图1.1 达\newline 芬奇系列医疗手术机器人 & 宋体五号字 & Times New Roman 11pt & 居中书写，段前空6磅，段后空12磅，单倍行距，图序与图题文字之间空一个汉字符宽度 \\
% \textbf{表题} & 表2.13飞\newline 行时间质谱装置 & 宋体五号字 & Times New Roman 11pt & 居中书写，段前空12磅，段后空6磅，行距为单倍行距，表序与表题文字之间空一个汉字符宽度。 \\
% \textbf{参考文献} & [1] 作者.\newline 文题…… & 宋体五号字 & Times New Roman 11pt & 行距采用固定值16磅，段前空3 磅，段后空0磅。采用悬挂格式，悬挂缩进2个汉字符或1个厘米。 \\
% \textbf{脚注} & 源于…… & 宋体小五号字 & Times New Roman 小五号 & 两端对齐格式，悬挂缩进1.5字符，段前后均空0pt，单倍行距。\\
% \end{table}

% \noindent\textbf{（八）量和单位}

% 严格执行国家标准GB 3100—1993、GB/T 3101—1993和GB/T 3102—1993有关量和单位的规定。单位名称的书写，可以采用国际通用符号，也可以用中文名称，但全文应统一，不得两种混用。

% \noindent\textbf{（九）有关图、表和表达式}

% 图、表和表达式一律采用阿拉伯数字编号，并按章编号，前一位数字为章的序号，后一位数字为本章内图、表或表达式的顺序号。两数字间用小数点“.”或半角横线“-”连接。例如“图2.1”或“图2-1”，“表3.1”或“表3-1”等。表达式在文字叙述中采用“式（3-1）”或“式（3.1）”形式，在编号中用“（3-1）”或“（3.1）”形式。若图或表中有附注，采用英文小写字母顺序编号，附注写在图或表的下方。

% 附录中图、表、表达式的编号，应与正文中的编号区分开，即在阿拉伯数码前冠以附录的编号，如附录A中的图和表，表示为“图A.1”，“表A.2”等。

% \noindent 1. 图

% 图包括曲线图、构造图、示意图、框图、流程图、记录图、地图、照片等。图应具有“自明性”，即只看图、图题和图例，不阅读正文，就可理解图意。图要精选，切忌与表及文字表述重复。图中的术语、符号、单位等应与正文表述中所用一致。

% 图应有编号和图题，例如：“图 2.1 发展中国家经济增长速度的比较（1960-2000）”。 图 2.1 是编号，代表“第2章第1个图”，以此类推，“发展中国家经济增长速度的比较（1960-2000）”是图题。图的编号与图题置于图下方，采用11pt字居中书写，汉字用宋体，英文和数字用Times New Roman体，段前空6磅，段后空12磅，行距为单倍行距，图的编号与图题文字之间空一个汉字符宽度。

% 图中标注的文字宜采用9～10.5pt，以能够清晰阅读为标准，且全文保持一致。专用名字代号、单位可采用外文表示，坐标轴题名、词组、描述性的词语均须采用中文。考虑到图的复制效果和成本等因素，图中不同序列的点、线、条块等宜使用不同形状、线型、填充图案等加以区分，尽量避免使用颜色区分。

% 如果一个图由两个或两个以上分图组成时，各分图分别以(a)、(b)、(c)……作为图序，并须有分图题。如果分图编号嵌在图中，字号可略大于图中标注文字的字号以示区别，并保持全文一致；如果置于图片下方，字号宜与图的编号和图题字号保持一致。

% 图宜紧置于首次引用该图的文字之后。图应尽可能显示在同一页（屏）。如图太宽，可逆时针方向旋转 90°放置。图页面积太大时，可分别配置在两页上，次页上应注明“（续）”，并注明图题（可省略），例如“图 2.1（续） 发展中国家经济增长速度的比较（1960-2000）”。

% 如需英文图名，应中英文对照，英文图的编号与图名另起一行放在中文下方。英文编号和内容应和中文一致，如“Fig 2.1 Comparison of economic growth rates indeveloping countries (1960-2000)”

% \noindent 2. 表

% 表应具有“自明性”。表中参数应标明量和单位的符号。表头中应标明量和单位表示符号，表中的数字后面不再加单位符号。建议采用三线表，表的上、下边线为单直线，线粗为1.5磅；第三条线为单直线，线粗为1磅。

% 表单元格中的文字一般应居中书写（上下居中，左右居中），不宜左右居中书写的，可采取两端对齐的方式书写。表单元格中的文字采用11pt字书写，汉字用宋体，英文和数字用Times New Roman体，单倍行距，段前空3磅，段后空3磅。

% 表应有编号与表题，例如：“表3.1 第四次全国经济普查数据（北京）”。 表3.1是编号，代表“第3章第1个表”，以此类推，“第四次全国经济普查数据（北京）”是表题。表的编号与表题置于表上方，采用11pt字居中书写，汉字用宋体，英文和数字用Times New Roman 体，段前空12磅，段后空6磅，行距为单倍行距，表序与表题文字之间空一个汉字符宽度。

% 如某个表需要转页接排，在随后的各页上应重复表的编号，编号后跟表题（可省略）和“（续）”，置于表上方，例如“表3.1（续） 第四次全国经济普查数据（北京）”，续表均应重复表头和关于单位的陈述。

% 若在表下方注明资料来源，则此部分用五号字，汉字用宋体，英文用Times New Roman 体，单倍行距，段前空6磅，段后空12磅。需要续表时，资料来源注明在续表之下。

% 如需英文表名，应中英文对照，英文表的编号与表名另起一行放在中文下方。英文编号和内容应和中文一致，如“Table 3.1 Data from the Fourth National Economic Census (Beijing)”。

% \noindent 3. 表达式

% 表达式主要是指数字表达式，例如数学表达式，也包括文字表达式。

% 表达式应另起一行，采用与正文相同的字号居中书写，或另起一段空两个汉字符书写，一旦采用了上述两种格式中的一种，全文都要使用同一种格式。表达式应有编号，编号应加括号置于表达式右边行末，编号与表达式之间不加任何连线。

% 较长的表达式必须转行时，应在“=”或者“+”“-”“×”“/”等运算符或者“]”“\}”等括号之后回行。上下行尽可能在“=”处对齐。

% 表达式采用Cambria Math或Times New Roman体，采用12pt字书写，行距为单倍行距，段前空6磅，段后空6磅。

% 当表达式不是独立成行书写时，应尽量将其高度降低为一行，例如，将分数线书写成“/”，将根号改为负指数，例如2\textsuperscript{-1/2}。

% \noindent\textbf{（十）参考文献}

% 参考文献是文中引用的有具体文字来源的文献集合，列出作者直接阅读过、在正文中被引用过、正式或非正式发表的刊物、文献及资料。参考文献的写法应遵循国家标准《信息与文献参考文献著录规则》（GB/T 7714—2015）；符合特定学科的通用范式，可使用APA或《清华大学学报（哲学社会科学版）》格式，且应全文统一，不能混用。参考文献一律放在论文结论后，不得放在各章之后。在论文正文中引用了参考文献的部位，须用上标标注[参考文献序号]。

% “参考文献”四个字的格式与章标题的格式相同。参考文献表的正文部分用五号字，中文用宋体，英文和数字用Times New Roman体，行距采用固定值16磅，段前空3磅，段后空0磅。采用悬挂格式，悬挂缩进2个汉字符或1厘米。

% 每一条文献的内容要尽量写在同一页内。遇有被迫分页的情况，可通过“留白”或微调本页行距的方式尽量将同一条文献内容放在一页。

% 关于参考文献国家标准《信息与文献参考文献著录规则》（GB/T 7714—2015）的著录格式以及在正文中的标注方法详见《清华大学综合论文训练写作规范（试行）》附件的《参考文献著录规则及注意事项》。

% \noindent\textbf{（十一）附录}

% 附录的格式与正文相同，并依顺序用大写字母 A，B，C，……编序号，如附录A，附录B，附录C，……。只有一个附录时也要编序号，即附录A。每个附录应有标题。附录序号与附录标题之间空一个汉字符。例如：“附录A 外文资料的调研阅读报告”。

% 附录中的图、表、数学表达式、参考文献等另行编序号，与正文分开，一律用阿拉伯数字编码，但在数码前冠以附录的序号，例如“图A.1”，“表B.2”，“式（C-3）”等。

% 附录内容分为以下两部分：

% 1、附录A

% 附录A为外文资料的调研阅读报告或书面翻译。调研阅读报告需附参考文献；书面翻译需注明外文资料原文的索引。标题为“外文资料的调研阅读报告”或“外文资料的书面翻译”。

% 调研阅读报告的参考文献（或书面翻译对应的外文资料的原文索引）格式与正文参考文献格式相同。

% 2、其他附录

% 其他附录是与论文内容密切相关、但编入正文又影响整篇论文编排的条理和逻辑性的资料，例如某些重要的数据表格、计算程序、统计表等，是论文主体的补充内容，可根据需要设置。其他附录序号为附录B，附录C，……。

% \noindent\textbf{（十二）致谢}

% 致谢包括内容如：对国家科学基金、资助研究工作的奖学金基金、合同单位、资助或支持的企业、组织或个人，对协助完成研究工作和提供便利条件的组织或个人，对在研究工作中提出建议和提供帮助的人，对给予转载和引用权的资料、图片、文献、研究思想和设想的所有者，对其他应感谢的组织和个人。

% 致谢单设一页。标题为“致谢”。内容部分采用宋体小四号字，行距用固定值20 磅，段前后0磅。

% \noindent\textbf{（十三）声明}

% 关于论文内容没有侵占他人著作权的声明，放在致谢页后，单独一页。标题为“声明”。内容为“本人郑重声明：所呈交的综合论文训练论文，是本人在导师指导下，独立进行研究工作所取得的成果。尽我所知，除文中已经注明引用的内容外，本论文的研究成果不包含任何他人享有著作权的内容。对本论文所涉及的研究工作做出贡献的其他个人和集体，均已在文中以明确方式标明。” 确认无误后，慎重签名。

% 此部分内容可以直接下载《清华大学综合论文训练写作规范（试行）》附件的WORD文档，相应地复制到论文中即可，在提交论文时作者必须签署姓名。

% \noindent\textbf{（十四）在学期间参加课题的研究成果}

% 指在本科阶段课题研究中获得的成果，如申请的专利或已正式发表和已有正式录用函的论文等。标题为“在学期间参加课题的研究成果”， 使用黑体三号字，居中书写，单倍行距，段前空24磅，段后空18磅。内容部分用宋体小四号字，行距采用固定值20磅，段前后0磅。

% 各种类型学术成果的书写格式与正文相同，书写要求如下。

% 1. 学术论文

% 参照参考文献书写，尚未刊载但已经接到正式录用函的学术论文加括号注明已被××××期刊录用。

% 2. 专著/译著

% 参照参考文献书写，尚未出版但已被出版社决定出版的专著/译著加括号注明出版社名称和预计出版时间。

% 3. 专利

% 参照参考文献书写，处于申请阶段的专利在专利号位置填写专利申请号，并加括号注明是专利申请号。

% 4. 作品

% 大致按以下方式书写：作者. 作品名称. 创作时间. 材料形式. 作品尺寸. 作品地点. 参展信息. 是否获奖等信息。

% 5. 研究报告

% 公开的研究报告参照参考文献书写。

% 6. 其他

% 按适当合理的方式书写。

% \noindent\textbf{（十五）综合论文训练记录表}

% 完整和翔实记录综合论文训练开题、中期检查、论文评阅、论文答辩各环节情况。装订于论文的最后。

% \noindent\textbf{（十六）页面设置}

% 论文页面设置如下：
% \begin{enumerate}
%   \item 封面：纸张规格、尺寸，A4（21厘米×29.7厘米）；页边距，上3.8厘米，下3.2厘米，左3厘米，右3厘米；装订线，0.2厘米，位置左。
%   \item 除封面外，其他页面：纸张规格、尺寸，A4（21厘米×29.7厘米）；页边距，上3厘米，下3厘米，左3厘米，右3厘米；装订线，0厘米。
% \end{enumerate}
% 注：论文除“封面”、“关于论文使用授权的说明”采用单面印刷之外，从摘要开始（包括摘要）后面的部分均采用A4幅面白色70克以上80 克以下（彩色插图页除外）纸张双面印刷，正文从另页右页开始。

% \noindent\textbf{（十七）页眉和页码}

% 页眉：无

% 页码：位于页面底端，居中书写。在第1章（引言或绪论）之前的部分，从前往后用大写罗马数字编排；从第 1 章（引言或绪论）开始，用阿拉伯数字连续编排。综合论文训练记录表无页码。

% \noindent\textbf{（十八）书脊的书写要求}

% 用仿宋字书写，字体大小根据论文的薄厚而定。书脊上方写论文题目，下方写作者姓名，距上下页边均为3cm。

% \noindent\textbf{（十九）其它说明}

% 论文的某些部分内容若为空，如：主要符号表、在学期间参加课题的研究成果、附录B等，则该部分不要作为空白页装订在论文里。论文封皮统一要求使用120克蓝色纸。

% \section{模板及相关说明}
% 本科生综合论文训练论文模板以附件形式单独存成文档，供同学们下载参考。如果模板中存在与本规范中的规定不符之处，以本规范中的文字叙述为准。

% \noindent 01 综合论文训练论文模板

% \noindent 02 关于论文使用授权的说明

% \noindent 03 声明

% \noindent 04 在学期间参加课题的研究成果

% \noindent 05 综合论文训练记录表

% \noindent 06 参考文献著录规则及注意事项

% \Acknowledgments%致谢
% 首先，需要感谢我的导师吴丹教授在我毕业设计阶段对我的悉心指导和帮助。吴丹教授在毕业设计阶段为我提供了详尽且细致的指导和建议，并且在我遇到挫折和困难时引导我、鼓励我，使我能够发现新的思路，并使我能够顺利完成毕业设计任务。

% 其次，我要感谢实验室的师兄师姐们，他们在我的毕业设计过程中给予了我很多帮助和支持。师兄师姐们在实验室中分享了他们的经验和技巧，帮助我更好地理解课题内容，并且在我遇到技术问题时给予了及时的解答和指导。尤其是王松师兄，经常在实验室里耐心地解答我的问题，帮助我解决了很多技术难题，让我在毕业设计过程中受益匪浅。以及，武诗睿师姐，她在我毕业设计写作方面给予了我很多指导和建议，帮助我更好地组织和表达我的研究成果，并使得我的写作更符合规范。

% 再次，我要感谢我的家人对我的支持和鼓励。家人一直以来都在背后默默地支持我，给予我无私的爱和关怀，让我能够专心致志地投入到学习和研究中。

% 最后，我要感谢我的同学和朋友，他们与我并肩作战，互相帮助，共同进步。我们一起度过了许多难忘的时光，分享了彼此的经验和见解，使得我的毕业设计过程更加充实和有趣。

% \Statement%声明

\end{document}
